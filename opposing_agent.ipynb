{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a71f69-f73f-4281-87c8-cc8c5affd609",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Constants\n",
    "TILE_SIZE = 24\n",
    "GRID_WIDTH = 30\n",
    "GRID_HEIGHT = 20\n",
    "SCREEN_WIDTH = TILE_SIZE * GRID_WIDTH\n",
    "SCREEN_HEIGHT = TILE_SIZE * GRID_HEIGHT\n",
    "FPS = 10  # Increased FPS for smoother animation\n",
    "NUM_ACTIONS = 4  # Number of actions (up, down, left, right)\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Fireboy and Watergirl Grid World\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    \"0\": (120, 66, 18),   # Brown - Empty\n",
    "    \"1\": (160, 82, 45),   # Light Brown - Platform\n",
    "    \"F\": (255, 0, 0),     # Fireboy - Red Heart\n",
    "    \"W\": (0, 0, 255),     # Watergirl - Blue Heart\n",
    "    \"G\": (0, 255, 0),     # Green Agent\n",
    "    \"RP\": (255, 0, 0),  # Red Poison - Now a block\n",
    "    \"BP\": (0, 0, 255),  # Blue Poison - Now a block\n",
    "    \"GP\": (0, 255, 0),  # Green Poison - Now a block\n",
    "    \"FG\": (255, 0, 0),    # Fireboy Goal\n",
    "    \"WG\": (0, 0, 255),    # Watergirl Goal\n",
    "    \"RB\": (255, 0, 0),    # Red Gem\n",
    "    \"BB\": (0, 0, 255),    # Blue Gem\n",
    "    \"YS\": (255, 255, 0),  # Yellow Slide\n",
    "    \"PS\": (128, 0, 128),  # Purple Slide\n",
    "    \"YB\": (255, 255, 0),  # Yellow Button\n",
    "    \"PB\": (128, 0, 128),  # Purple Button\n",
    "}\n",
    "\n",
    "BASE_COLOR = COLORS[\"0\"]\n",
    "\n",
    "# Original grid layout - THIS WILL NEVER BE MODIFIED\n",
    "ORIGINAL_GRID = [[\"0\" for _ in range(GRID_WIDTH)] for _ in range(GRID_HEIGHT)]\n",
    "\n",
    "# Place sample tiles\n",
    "ORIGINAL_GRID[18][1] = \"F\"    # Fireboy\n",
    "ORIGINAL_GRID[15][1] = \"W\"    # Watergirl\n",
    "ORIGINAL_GRID[2][19] = \"G\"    # Green agent\n",
    "\n",
    "# Poison tiles\n",
    "for x in range(10, 13):\n",
    "    ORIGINAL_GRID[18][x] = \"RP\"\n",
    "\n",
    "for x in range(17, 20):\n",
    "    ORIGINAL_GRID[18][x] = \"BP\"\n",
    "\n",
    "for x in range(18, 21):\n",
    "    ORIGINAL_GRID[14][x] = \"GP\"\n",
    "\n",
    "# Gems\n",
    "ORIGINAL_GRID[17][11] = ORIGINAL_GRID[8][10] = \"RB\"\n",
    "ORIGINAL_GRID[17][18] = ORIGINAL_GRID[10][20] = \"BB\"\n",
    "\n",
    "# Goals\n",
    "ORIGINAL_GRID[2][26] = \"FG\"\n",
    "ORIGINAL_GRID[2][28] = \"WG\"\n",
    "\n",
    "# Slides\n",
    "for x in range(1, 4):\n",
    "    ORIGINAL_GRID[9][x] = \"YS\"  # Yellow slides\n",
    "\n",
    "for x in range(26, 29):\n",
    "    ORIGINAL_GRID[6][x] = \"PS\"  # Purple slides\n",
    "\n",
    "# Buttons\n",
    "ORIGINAL_GRID[8][6] = ORIGINAL_GRID[12][9] = \"YB\"  # Yellow buttons\n",
    "ORIGINAL_GRID[8][14] = ORIGINAL_GRID[5][22] = \"PB\"  # Purple buttons\n",
    "\n",
    "# Assigning 1's for walls and platforms\n",
    "for i in range(0, 30):\n",
    "    ORIGINAL_GRID[0][i] = \"1\"\n",
    "    ORIGINAL_GRID[19][i] = \"1\"\n",
    "\n",
    "for i in range(0, 20):\n",
    "    ORIGINAL_GRID[i][0] = \"1\"\n",
    "    ORIGINAL_GRID[i][29] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[1][1] = ORIGINAL_GRID[1][2] = \"1\"\n",
    "for i in range(4, 29):\n",
    "    ORIGINAL_GRID[3][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[4][1] = ORIGINAL_GRID[4][2] = ORIGINAL_GRID[5][1] = ORIGINAL_GRID[5][2] = \"1\"\n",
    "\n",
    "for i in range(1, 26):\n",
    "    ORIGINAL_GRID[6][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[7][18] = ORIGINAL_GRID[7][19] = ORIGINAL_GRID[7][20] = \"1\"\n",
    "ORIGINAL_GRID[8][18] = ORIGINAL_GRID[8][19] = ORIGINAL_GRID[8][20] = \"1\"\n",
    "\n",
    "for i in range(4, 16):\n",
    "    ORIGINAL_GRID[9][i] = \"1\"\n",
    "\n",
    "for i in range(17, 22):\n",
    "    ORIGINAL_GRID[11][i] = \"1\"\n",
    "\n",
    "for i in range(22, 29):\n",
    "    ORIGINAL_GRID[10][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[10][16] = \"1\"\n",
    "ORIGINAL_GRID[11][26] = ORIGINAL_GRID[11][27] = ORIGINAL_GRID[11][28] = \"1\"\n",
    "ORIGINAL_GRID[12][26] = ORIGINAL_GRID[12][27] = ORIGINAL_GRID[12][28] = \"1\"\n",
    "\n",
    "for i in range(1, 13):\n",
    "    ORIGINAL_GRID[13][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[14][13] = \"1\"\n",
    "\n",
    "for i in range(14, 27):\n",
    "    ORIGINAL_GRID[15][i] = \"1\"\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ORIGINAL_GRID[16][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[17][27] = ORIGINAL_GRID[17][28] = ORIGINAL_GRID[18][27] = ORIGINAL_GRID[18][28] = \"1\"\n",
    "\n",
    "# Copy original grid to working grid\n",
    "grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "\n",
    "# Track slide positions and active buttons\n",
    "yellow_slides_positions = []\n",
    "purple_slides_positions = []\n",
    "yellow_buttons_active = False\n",
    "purple_buttons_active = False\n",
    "slide_animation_counter = 0\n",
    "SLIDE_ANIMATION_SPEED = 15  # Controls how fast slides move\n",
    "\n",
    "# Initialize slide positions\n",
    "def initialize_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions\n",
    "    yellow_slides_positions = []\n",
    "    purple_slides_positions = []\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if ORIGINAL_GRID[y][x] == \"YS\":\n",
    "                yellow_slides_positions.append((y, x))\n",
    "            elif ORIGINAL_GRID[y][x] == \"PS\":\n",
    "                purple_slides_positions.append((y, x))\n",
    "\n",
    "# Create grid maps for environment features\n",
    "def create_grid_map(feature_type):\n",
    "    \"\"\"Create a boolean grid map for different environmental features\"\"\"\n",
    "    grid_map = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=bool)\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            cell = grid[y][x]\n",
    "            if feature_type == 'wall' and cell == \"1\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_poison' and cell == \"RP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_poison' and cell == \"BP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'green_poison' and cell == \"GP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_gem' and cell == \"RB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_gem' and cell == \"BB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'fire_goal' and cell == \"FG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'water_goal' and cell == \"WG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_button' and cell == \"YB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_button' and cell == \"PB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_slide' and cell == \"YS\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_slide' and cell == \"PS\":\n",
    "                grid_map[y][x] = True\n",
    "    \n",
    "    return grid_map\n",
    "\n",
    "# Create all grid maps\n",
    "def initialize_environment_maps():\n",
    "    wall_map = create_grid_map('wall')\n",
    "    red_poison_map = create_grid_map('red_poison')\n",
    "    blue_poison_map = create_grid_map('blue_poison')\n",
    "    green_poison_map = create_grid_map('green_poison')\n",
    "    red_gem_map = create_grid_map('red_gem')\n",
    "    blue_gem_map = create_grid_map('blue_gem')\n",
    "    fire_goal_map = create_grid_map('fire_goal')\n",
    "    water_goal_map = create_grid_map('water_goal')\n",
    "    yellow_button_map = create_grid_map('yellow_button')\n",
    "    purple_button_map = create_grid_map('purple_button')\n",
    "    yellow_slide_map = create_grid_map('yellow_slide')\n",
    "    purple_slide_map = create_grid_map('purple_slide')\n",
    "    \n",
    "    # Create combined maps\n",
    "    poison_map = red_poison_map | blue_poison_map | green_poison_map\n",
    "    \n",
    "    # Create environment dictionary\n",
    "    environment = {\n",
    "        'wall': wall_map,\n",
    "        'poison': poison_map,\n",
    "        'red_poison': red_poison_map,\n",
    "        'blue_poison': blue_poison_map,\n",
    "        'green_poison': green_poison_map,\n",
    "        'red_gems': red_gem_map.copy(),\n",
    "        'blue_gems': blue_gem_map.copy(),\n",
    "        'fire_goal': fire_goal_map,\n",
    "        'water_goal': water_goal_map,\n",
    "        'yellow_button': yellow_button_map,\n",
    "        'purple_button': purple_button_map,\n",
    "        'yellow_slide': yellow_slide_map,\n",
    "        'purple_slide': purple_slide_map\n",
    "    }\n",
    "    \n",
    "    return environment\n",
    "\n",
    "# Agent position setup\n",
    "def get_initial_positions():\n",
    "    fireboy_pos = None\n",
    "    watergirl_pos = None\n",
    "    green_pos = None\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"F\":\n",
    "                fireboy_pos = (y, x)\n",
    "            elif grid[y][x] == \"W\":\n",
    "                watergirl_pos = (y, x)\n",
    "            elif grid[y][x] == \"G\":\n",
    "                green_pos = (y, x)\n",
    "    \n",
    "    return fireboy_pos, watergirl_pos, green_pos\n",
    "\n",
    "# Draw Function\n",
    "def draw_grid():\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            tile = grid[y][x]\n",
    "            color = COLORS.get(tile, BASE_COLOR)\n",
    "\n",
    "            # Agents: Heart shape\n",
    "            if tile in [\"F\", \"W\", \"G\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + 6),\n",
    "                    (x*TILE_SIZE + 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE - 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                ])\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + 2*TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "\n",
    "            # Gems: Rhombus\n",
    "            elif tile in [\"RB\", \"BB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2)\n",
    "                ])\n",
    "\n",
    "            # Buttons: Circle\n",
    "            elif tile in [\"YB\", \"PB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE//2), TILE_SIZE//4)\n",
    "\n",
    "            # Goals: Triangle\n",
    "            elif tile in [\"FG\", \"WG\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE)\n",
    "                ])\n",
    "\n",
    "            # Slides: Full tile fill\n",
    "            elif tile in [\"YS\", \"PS\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                \n",
    "            # Poison: Now blocks with specific colors\n",
    "            elif tile in [\"RP\", \"BP\", \"GP\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Default or walls\n",
    "            else:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Draw the black border\n",
    "            pygame.draw.rect(screen, (0, 0, 0), (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE), 1)\n",
    "\n",
    "# Define the policy network for cooperative agents\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return torch.softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "# Define the DQN for the green agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values (not softmax)\n",
    "\n",
    "# Initialize policies for cooperative agents (Fireboy and Watergirl)\n",
    "policy_f = PolicyNetwork(input_size=10, output_size=NUM_ACTIONS)\n",
    "policy_w = PolicyNetwork(input_size=10, output_size=NUM_ACTIONS)\n",
    "\n",
    "# Initialize DQN for the green adversarial agent with larger input size to include more opponent information\n",
    "# Increased input size for more complete state representation\n",
    "dqn = DQN(input_size=20, output_size=NUM_ACTIONS)\n",
    "target_dqn = DQN(input_size=20, output_size=NUM_ACTIONS)  # Target network for stable learning\n",
    "target_dqn.load_state_dict(dqn.state_dict())  # Initially the same as the online network\n",
    "\n",
    "# Optimizers\n",
    "optimizer_f = optim.Adam(policy_f.parameters(), lr=0.001)\n",
    "optimizer_w = optim.Adam(policy_w.parameters(), lr=0.001)\n",
    "optimizer_dqn = optim.Adam(dqn.parameters(), lr=0.0005)  # Lower learning rate for stability\n",
    "\n",
    "# DQN hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE = 10  # How often to update target network\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "# Experience replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, reward, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Initialize replay memory\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Initialize agent state\n",
    "def reset_environment():\n",
    "    # Reset grid to original state first\n",
    "    global grid, yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "    grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "    yellow_buttons_active = False\n",
    "    purple_buttons_active = False\n",
    "    slide_animation_counter = 0\n",
    "    \n",
    "    # Initialize slides\n",
    "    initialize_slides()\n",
    "    \n",
    "    # Get fresh positions from the reset grid\n",
    "    fireboy_pos, watergirl_pos, green_pos = get_initial_positions()\n",
    "    \n",
    "    # Create agent dictionaries with position and other properties\n",
    "    red_agent = {'pos': fireboy_pos, 'color': 'red', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    blue_agent = {'pos': watergirl_pos, 'color': 'blue', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    green_agent = {'pos': green_pos, 'color': 'green', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    \n",
    "    # Create fresh environment maps from the reset grid\n",
    "    environment = initialize_environment_maps()\n",
    "    \n",
    "    return red_agent, blue_agent, green_agent, environment\n",
    "\n",
    "# Handle slide movement\n",
    "def update_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions, grid, slide_animation_counter\n",
    "    \n",
    "    # Clear previous slide positions from grid\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"YS\" or grid[y][x] == \"PS\":\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Update yellow slides\n",
    "    new_yellow_positions = []\n",
    "    for y, x in yellow_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if yellow_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_yellow_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"YS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in yellow_slides_positions):\n",
    "                    current_pos = [pos for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_yellow_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((y, x))\n",
    "                grid[y][x] = \"YS\"\n",
    "    \n",
    "    # Update purple slides\n",
    "    new_purple_positions = []\n",
    "    for y, x in purple_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if purple_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_purple_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"PS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in purple_slides_positions):\n",
    "                    current_pos = [pos for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_purple_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((y, x))\n",
    "                grid[y][x] = \"PS\"\n",
    "    \n",
    "    # Update slide positions for next frame\n",
    "    yellow_slides_positions = new_yellow_positions\n",
    "    purple_slides_positions = new_purple_positions\n",
    "    \n",
    "    # Update animation counter\n",
    "    if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "        slide_animation_counter += 1\n",
    "\n",
    "# Get state representation for policy networks (cooperative agents)\n",
    "def get_state_representation(agent, other_agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    oy, ox = other_agent['pos']\n",
    "    \n",
    "    # Simple state representation: relative positions and nearby features\n",
    "    state = [\n",
    "        y / GRID_HEIGHT,                  # Normalized y position\n",
    "        x / GRID_WIDTH,                   # Normalized x position\n",
    "        (oy - y) / GRID_HEIGHT,           # Relative y position to other agent\n",
    "        (ox - x) / GRID_WIDTH,            # Relative x position to other agent\n",
    "        1.0 if agent['is_pressing_button'] else 0.0,  # Is agent pressing button\n",
    "        1.0 if other_agent['is_pressing_button'] else 0.0,  # Is other agent pressing button\n",
    "        1.0 if agent['goal_reached'] else 0.0,  # Has agent reached goal\n",
    "        1.0 if other_agent['goal_reached'] else 0.0,  # Has other agent reached goal\n",
    "        agent['collected_gems'] / 2.0,    # Normalized collected gems (assuming max 2)\n",
    "        other_agent['collected_gems'] / 2.0  # Other agent's collected gems\n",
    "    ]\n",
    "    \n",
    "    # Use torch.tensor with requires_grad=True for policy gradients\n",
    "    return torch.tensor(state, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# Enhanced state representation for DQN (green agent)\n",
    "def get_dqn_state_representation(green_agent, red_agent, blue_agent, environment):\n",
    "    gy, gx = green_agent['pos']\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    \n",
    "    # Find locations of goals\n",
    "    fire_goal_positions = np.where(environment['fire_goal'])\n",
    "    water_goal_positions = np.where(environment['water_goal'])\n",
    "    fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0] if len(fire_goal_positions[0]) > 0 else (0, 0)\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0] if len(water_goal_positions[0]) > 0 else (0, 0)\n",
    "    \n",
    "    # Calculate distances to fire/water agents\n",
    "    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Calculate distances from fire/water to their goals\n",
    "    red_to_goal = np.sqrt((ry - fgy)**2 + (rx - fgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    blue_to_goal = np.sqrt((by - wgy)**2 + (bx - wgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Create a comprehensive state representation\n",
    "    state = [\n",
    "        gy / GRID_HEIGHT,                  # Green agent y position\n",
    "        gx / GRID_WIDTH,                   # Green agent x position\n",
    "        ry / GRID_HEIGHT,                  # Red agent y position\n",
    "        rx / GRID_WIDTH,                   # Red agent x position\n",
    "        by / GRID_HEIGHT,                  # Blue agent y position\n",
    "        bx / GRID_WIDTH,                   # Blue agent x position\n",
    "        dist_to_red,                       # Distance to red agent\n",
    "        dist_to_blue,                      # Distance to blue agent\n",
    "        red_to_goal,                       # Red agent's distance to goal\n",
    "        blue_to_goal,                      # Blue agent's distance to goal\n",
    "        1.0 if red_agent['is_pressing_button'] else 0.0,  # Is red agent pressing button\n",
    "        1.0 if blue_agent['is_pressing_button'] else 0.0,  # Is blue agent pressing button\n",
    "        1.0 if red_agent['goal_reached'] else 0.0,        # Has red reached goal\n",
    "        1.0 if blue_agent['goal_reached'] else 0.0,       # Has blue reached goal\n",
    "        red_agent['collected_gems'] / 2.0,                # Red collected gems\n",
    "        blue_agent['collected_gems'] / 2.0,               # Blue collected gems\n",
    "        fgy / GRID_HEIGHT,                 # Fire goal y position\n",
    "        fgx / GRID_WIDTH,                  # Fire goal x position\n",
    "        wgy / GRID_HEIGHT,                 # Water goal y position\n",
    "        wgx / GRID_WIDTH,                  # Water goal x position\n",
    "    ]\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Check if agent is on a button and update button state\n",
    "def check_button_press(agent, environment):\n",
    "    global yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "    \n",
    "    y, x = agent['pos']\n",
    "    was_pressing = agent['is_pressing_button']\n",
    "    \n",
    "    if agent['color'] == 'red' and environment['yellow_button'][y][x]:\n",
    "        agent['is_pressing_button'] = True\n",
    "        yellow_buttons_active = True\n",
    "        slide_animation_counter = 0  # Reset animation counter when button is pressed\n",
    "    elif agent['color'] == 'blue' and environment['purple_button'][y][x]:\n",
    "        agent['is_pressing_button'] = True\n",
    "        purple_buttons_active = True\n",
    "        slide_animation_counter = 0  # Reset animation counter when button is pressed\n",
    "    else:\n",
    "        agent['is_pressing_button'] = False\n",
    "        \n",
    "    # If the agent stopped pressing a button, reset that button state\n",
    "    if was_pressing and not agent['is_pressing_button']:\n",
    "        if agent['color'] == 'red':\n",
    "            yellow_buttons_active = False\n",
    "            slide_animation_counter = 0\n",
    "        elif agent['color'] == 'blue':\n",
    "            purple_buttons_active = False\n",
    "            slide_animation_counter = 0\n",
    "\n",
    "# Check if agent has collected a gem - now color-specific and gems disappear after collection\n",
    "def check_gem_collection(agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    if agent['color'] == 'red' and environment['red_gems'][y][x]:\n",
    "        environment['red_gems'][y][x] = False  # Remove collected gem\n",
    "        grid[y][x] = \"0\"  # Update visual grid to remove the gem\n",
    "        agent['collected_gems'] += 1\n",
    "        return True\n",
    "    elif agent['color'] == 'blue' and environment['blue_gems'][y][x]:\n",
    "        environment['blue_gems'][y][x] = False  # Remove collected gem\n",
    "        grid[y][x] = \"0\"  # Update visual grid to remove the gem\n",
    "        agent['collected_gems'] += 1\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Check if agent has reached their goal\n",
    "def check_goal_reached(agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    if agent['color'] == 'red' and environment['fire_goal'][y][x]:\n",
    "        agent['goal_reached'] = True\n",
    "        return True\n",
    "    elif agent['color'] == 'blue' and environment['water_goal'][y][x]:\n",
    "        agent['goal_reached'] = True\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Check if agent is in poison - now with specific fatal poison types!\n",
    "def check_poison(agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    \n",
    "    # Red agent (Fireboy) dies in blue and green poison\n",
    "    if agent['color'] == 'red':\n",
    "        if environment['blue_poison'][y][x] or environment['green_poison'][y][x]:\n",
    "            agent['died'] = True\n",
    "            return True\n",
    "    \n",
    "    # Blue agent (Watergirl) dies in red and green poison\n",
    "    elif agent['color'] == 'blue':\n",
    "        if environment['red_poison'][y][x] or environment['green_poison'][y][x]:\n",
    "            agent['died'] = True\n",
    "            return True\n",
    "    \n",
    "    # Green agent dies in all poisons\n",
    "    elif agent['color'] == 'green':\n",
    "        if environment['poison'][y][x]:\n",
    "            agent['died'] = True\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "# Perform an action and return new state, reward, and done flag\n",
    "def step_environment(red_agent, blue_agent, green_agent, actions, environment):\n",
    "    r_action, b_action, g_action = actions\n",
    "    r_reward, b_reward, g_reward = 0, 0, 0\n",
    "    \n",
    "    # Action mapping: 0=up, 1=right, 2=down, 3=left\n",
    "    action_mapping = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # (dy, dx)\n",
    "    \n",
    "    # Execute actions for all agents\n",
    "    execute_action(red_agent, r_action, action_mapping, environment)\n",
    "    execute_action(blue_agent, b_action, action_mapping, environment)\n",
    "    execute_action(green_agent, g_action, action_mapping, environment)\n",
    "    \n",
    "    # Update grid with new agent positions\n",
    "    update_grid_with_agents(red_agent, blue_agent, green_agent)\n",
    "    \n",
    "    # Check for button presses\n",
    "    check_button_press(red_agent, environment)\n",
    "    check_button_press(blue_agent, environment)\n",
    "    \n",
    "    # Update slides based on button states\n",
    "    update_slides()\n",
    "    \n",
    "    # Check for gem collection\n",
    "    if check_gem_collection(red_agent, environment):\n",
    "        r_reward += 10  # Reward for collecting a gem\n",
    "    if check_gem_collection(blue_agent, environment):\n",
    "        b_reward += 10  # Reward for collecting a gem\n",
    "    \n",
    "    # Check for goal reached\n",
    "    if check_goal_reached(red_agent, environment):\n",
    "        r_reward += 50  # Big reward for reaching goal\n",
    "    if check_goal_reached(blue_agent, environment):\n",
    "        b_reward += 50  # Big reward for reaching goal\n",
    "    \n",
    "    # Check for poison\n",
    "    if check_poison(red_agent, environment):\n",
    "        r_reward -= 100  # Penalty for dying\n",
    "    if check_poison(blue_agent, environment):\n",
    "        b_reward -= 100  # Penalty for dying\n",
    "    if check_poison(green_agent, environment):\n",
    "        g_reward -= 10  # Smaller penalty for adversary dying\n",
    "    \n",
    "    # Adversarial reward: green gets positive reward when fire/water dies\n",
    "    if red_agent['died'] or blue_agent['died']:\n",
    "        g_reward += 50\n",
    "    \n",
    "    # Proximity penalty/reward for green agent (rewards being close to heroes)\n",
    "    gy, gx = green_agent['pos']\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    \n",
    "    # Calculate distances\n",
    "    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2)\n",
    "    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2)\n",
    "    \n",
    "    # Reward for being close to heroes\n",
    "    proximity_reward = max(0, 10 - dist_to_red) + max(0, 10 - dist_to_blue)\n",
    "    g_reward += proximity_reward * 0.5\n",
    "    \n",
    "    # Cooperative reward: shared between red and blue\n",
    "    team_reward = r_reward + b_reward\n",
    "    r_reward = team_reward\n",
    "    b_reward = team_reward\n",
    "    \n",
    "    # Check if game is over\n",
    "    game_over = (red_agent['goal_reached'] and blue_agent['goal_reached']) or \\\n",
    "                red_agent['died'] or blue_agent['died'] or \\\n",
    "                green_agent['died']\n",
    "    \n",
    "    return (r_reward, b_reward, g_reward), game_over\n",
    "\n",
    "# Execute an action for a single agent\n",
    "def execute_action(agent, action, action_mapping, environment):\n",
    "    if agent['goal_reached'] or agent['died']:\n",
    "        return  # Don't move if agent is done\n",
    "    \n",
    "    y, x = agent['pos']\n",
    "    dy, dx = action_mapping[action]\n",
    "    new_y, new_x = y + dy, x + dx\n",
    "    \n",
    "    # Check if the move is valid (not a wall)\n",
    "    if 0 <= new_y < GRID_HEIGHT and 0 <= new_x < GRID_WIDTH and not environment['wall'][new_y][new_x]:\n",
    "        agent['pos'] = (new_y, new_x)\n",
    "\n",
    "# Update the grid with agent positions\n",
    "def update_grid_with_agents(red_agent, blue_agent, green_agent):\n",
    "    # First clear previous agent positions\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] in [\"F\", \"W\", \"G\"]:\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Then place agents at their new positions if they're not dead\n",
    "    if not red_agent['died']:\n",
    "        ry, rx = red_agent['pos']\n",
    "        grid[ry][rx] = \"F\"\n",
    "    \n",
    "    if not blue_agent['died']:\n",
    "        by, bx = blue_agent['pos']\n",
    "        grid[by][bx] = \"W\"\n",
    "    \n",
    "    if not green_agent['died']:\n",
    "        gy, gx = green_agent['pos']\n",
    "        grid[gy][gx] = \"G\"\n",
    "\n",
    "# Select action with epsilon-greedy policy for DQN\n",
    "def select_dqn_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, NUM_ACTIONS - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values and return action with highest Q-value\n",
    "            q_values = dqn(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "# Train the DQN with batch from replay memory\n",
    "def optimize_dqn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch\n",
    "    batch = list(zip(*transitions))\n",
    "    \n",
    "    # Extract components\n",
    "    state_batch = torch.cat(batch[0])\n",
    "    action_batch = torch.tensor(batch[1], dtype=torch.long).unsqueeze(1)\n",
    "    next_state_batch = torch.cat(batch[2])\n",
    "    reward_batch = torch.tensor(batch[3], dtype=torch.float).unsqueeze(1)\n",
    "    done_batch = torch.tensor(batch[4], dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
    "    state_action_values = dqn(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states using target network\n",
    "    with torch.no_grad():\n",
    "        next_state_values = target_dqn(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "    \n",
    "    # Compute the expected Q values: r + Î³ * max_a' Q(s', a')\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values * (1 - done_batch))\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer_dqn.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in dqn.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)  # Gradient clipping\n",
    "    optimizer_dqn.step()\n",
    "\n",
    "# Training loop function\n",
    "def train():\n",
    "    global epsilon\n",
    "    episode_rewards = []\n",
    "    win_rate = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    for episode in range(1000):  # Train for 1000 episodes\n",
    "        # Reset environment and get initial state\n",
    "        red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "        episode_reward_r = 0\n",
    "        episode_reward_b = 0\n",
    "        episode_reward_g = 0\n",
    "        won = False\n",
    "        \n",
    "        for frame in range(100):  # Max 100 frames per episode\n",
    "            # Select actions\n",
    "            red_state = get_state_representation(red_agent, blue_agent, environment)\n",
    "            blue_state = get_state_representation(blue_agent, red_agent, environment)\n",
    "            green_state = get_dqn_state_representation(green_agent, red_agent, blue_agent, environment)\n",
    "            \n",
    "            # For red and blue agents: use policy networks\n",
    "            red_probs = policy_f(red_state)\n",
    "            red_action = torch.multinomial(red_probs, 1).item()\n",
    "            \n",
    "            blue_probs = policy_w(blue_state)\n",
    "            blue_action = torch.multinomial(blue_probs, 1).item()\n",
    "            \n",
    "            # For green agent: use epsilon-greedy from DQN\n",
    "            epsilon = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                      math.exp(-1. * frame_count / EPSILON_DECAY)\n",
    "            green_action = select_dqn_action(green_state, epsilon)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            (r_reward, b_reward, g_reward), done = step_environment(\n",
    "                red_agent, blue_agent, green_agent, \n",
    "                (red_action, blue_action, green_action), \n",
    "                environment\n",
    "            )\n",
    "            \n",
    "            # Get next state\n",
    "            next_red_state = get_state_representation(red_agent, blue_agent, environment)\n",
    "            next_blue_state = get_state_representation(blue_agent, red_agent, environment)\n",
    "            next_green_state = get_dqn_state_representation(green_agent, red_agent, blue_agent, environment)\n",
    "            \n",
    "            # Store transition in replay memory for green agent\n",
    "            memory.push(green_state, green_action, next_green_state, g_reward, done)\n",
    "            \n",
    "            # Train DQN\n",
    "            optimize_dqn()\n",
    "            \n",
    "            # Update episode rewards\n",
    "            episode_reward_r += r_reward\n",
    "            episode_reward_b += b_reward\n",
    "            episode_reward_g += g_reward\n",
    "            \n",
    "            # Check if game was won\n",
    "            if red_agent['goal_reached'] and blue_agent['goal_reached']:\n",
    "                won = True\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Update target network periodically\n",
    "            if frame_count % TARGET_UPDATE == 0:\n",
    "                target_dqn.load_state_dict(dqn.state_dict())\n",
    "            \n",
    "            # Update display and handle events\n",
    "            draw_grid()\n",
    "            pygame.display.flip()\n",
    "            clock.tick(FPS)\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Record episode statistics\n",
    "        episode_rewards.append((episode_reward_r, episode_reward_b, episode_reward_g))\n",
    "        win_rate.append(1 if won else 0)\n",
    "        \n",
    "        # Print episode information every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode: {episode}, Red Reward: {episode_reward_r}, Blue Reward: {episode_reward_b}, Green Reward: {episode_reward_g}\")\n",
    "            print(f\"Win Rate (last 10): {sum(win_rate[-10:]) / 10}\")\n",
    "            \n",
    "        # Save models every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            torch.save(policy_f.state_dict(), \"policy_f.pth\")\n",
    "            torch.save(policy_w.state_dict(), \"policy_w.pth\")\n",
    "            torch.save(dqn.state_dict(), \"dqn.pth\")\n",
    "\n",
    "# Main game loop\n",
    "def main():\n",
    "    running = True\n",
    "    red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "    \n",
    "    # Training mode flag\n",
    "    training_mode = False\n",
    "    \n",
    "    while running:\n",
    "        # Handle Pygame events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_t:\n",
    "                    # Toggle training mode\n",
    "                    training_mode = not training_mode\n",
    "                    print(f\"Training mode: {'ON' if training_mode else 'OFF'}\")\n",
    "                elif event.key == pygame.K_r:\n",
    "                    # Reset environment\n",
    "                    red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "                elif event.key == pygame.K_SPACE and training_mode:\n",
    "                    # Start training\n",
    "                    train()\n",
    "        \n",
    "        # Interactive mode (non-training)\n",
    "        if not training_mode:\n",
    "            # Get keyboard input for manual control\n",
    "            keys = pygame.key.get_pressed()\n",
    "            \n",
    "            # Red agent controls (WASD)\n",
    "            red_action = None\n",
    "            if keys[pygame.K_w]:\n",
    "                red_action = 0  # Up\n",
    "            elif keys[pygame.K_d]:\n",
    "                red_action = 1  # Right\n",
    "            elif keys[pygame.K_s]:\n",
    "                red_action = 2  # Down\n",
    "            elif keys[pygame.K_a]:\n",
    "                red_action = 3  # Left\n",
    "            \n",
    "            # Blue agent controls (Arrow keys)\n",
    "            blue_action = None\n",
    "            if keys[pygame.K_UP]:\n",
    "                blue_action = 0  # Up\n",
    "            elif keys[pygame.K_RIGHT]:\n",
    "                blue_action = 1  # Right\n",
    "            elif keys[pygame.K_DOWN]:\n",
    "                blue_action = 2  # Down\n",
    "            elif keys[pygame.K_LEFT]:\n",
    "                blue_action = 3  # Left\n",
    "            \n",
    "            # Get green agent action from model\n",
    "            green_state = get_dqn_state_representation(green_agent, red_agent, blue_agent, environment)\n",
    "            green_action = select_dqn_action(green_state, 0.1)  # Small exploration rate\n",
    "            \n",
    "            # Take actions if provided\n",
    "            if red_action is not None and blue_action is not None:\n",
    "                step_environment(red_agent, blue_agent, green_agent, \n",
    "                                (red_action, blue_action, green_action), environment)\n",
    "        \n",
    "        # Update slide positions\n",
    "        update_slides()\n",
    "        \n",
    "        # Draw the grid\n",
    "        screen.fill((0, 0, 0))\n",
    "        draw_grid()\n",
    "        \n",
    "        # Display game over message if applicable\n",
    "        if red_agent['died'] or blue_agent['died']:\n",
    "            font = pygame.font.SysFont(None, 55)\n",
    "            text = font.render(\"Game Over! Press R to restart\", True, (255, 255, 255))\n",
    "            screen.blit(text, (SCREEN_WIDTH/2 - text.get_width()/2, SCREEN_HEIGHT/2 - text.get_height()/2))\n",
    "        elif red_agent['goal_reached'] and blue_agent['goal_reached']:\n",
    "            font = pygame.font.SysFont(None, 55)\n",
    "            text = font.render(\"You Win! Press R to restart\", True, (255, 255, 255))\n",
    "            screen.blit(text, (SCREEN_WIDTH/2 - text.get_width()/2, SCREEN_HEIGHT/2 - text.get_height()/2))\n",
    "        \n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        clock.tick(FPS)\n",
    "    \n",
    "    pygame.quit()\n",
    "    sys.exit()\n",
    "\n",
    "# Run the game\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cca20fd-63bc-4945-b871-d599fbec761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Srinu\\AppData\\Local\\Temp\\ipykernel_26432\\2148696756.py:1066: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(f_log_prob),\n",
      "C:\\Users\\Srinu\\AppData\\Local\\Temp\\ipykernel_26432\\2148696756.py:1076: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(w_log_prob),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x25 and 24x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1118\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1117\u001b[0m     initialize_slides()\n\u001b[1;32m-> 1118\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[3], line 1091\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1089\u001b[0m train_mappo(memory_f, policy_f, optimizer_f)\n\u001b[0;32m   1090\u001b[0m train_mappo(memory_w, policy_w, optimizer_w)\n\u001b[1;32m-> 1091\u001b[0m train_dqn()\n\u001b[0;32m   1093\u001b[0m episode_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# Update target network\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 926\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[1;34m()\u001b[0m\n\u001b[0;32m    923\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch_dones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# Compute Q values\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m q_values \u001b[38;5;241m=\u001b[39m dqn(states)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# Compute next Q values with target network\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 352\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m--> 352\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(state))\n\u001b[0;32m    353\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x25 and 24x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "TILE_SIZE = 24\n",
    "GRID_WIDTH = 30\n",
    "GRID_HEIGHT = 20\n",
    "SCREEN_WIDTH = TILE_SIZE * GRID_WIDTH\n",
    "SCREEN_HEIGHT = TILE_SIZE * GRID_HEIGHT\n",
    "FPS = 10  # Increased FPS for smoother animation\n",
    "NUM_ACTIONS = 4  # Number of actions (up, down, left, right)\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Fireboy and Watergirl Grid World\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    \"0\": (120, 66, 18),   # Brown - Empty\n",
    "    \"1\": (160, 82, 45),   # Light Brown - Platform\n",
    "    \"F\": (255, 0, 0),     # Fireboy - Red Heart\n",
    "    \"W\": (0, 0, 255),     # Watergirl - Blue Heart\n",
    "    \"G\": (0, 255, 0),     # Green Agent\n",
    "    \"RP\": (255, 0, 0),  # Red Poison - Now a block\n",
    "    \"BP\": (0, 0, 255),  # Blue Poison - Now a block\n",
    "    \"GP\": (0, 255, 0),  # Green Poison - Now a block\n",
    "    \"FG\": (255, 0, 0),    # Fireboy Goal\n",
    "    \"WG\": (0, 0, 255),    # Watergirl Goal\n",
    "    \"RB\": (255, 0, 0),    # Red Gem\n",
    "    \"BB\": (0, 0, 255),    # Blue Gem\n",
    "    \"YS\": (255, 255, 0),  # Yellow Slide\n",
    "    \"PS\": (128, 0, 128),  # Purple Slide\n",
    "    \"YB\": (255, 255, 0),  # Yellow Button\n",
    "    \"PB\": (128, 0, 128),  # Purple Button\n",
    "}\n",
    "\n",
    "BASE_COLOR = COLORS[\"0\"]\n",
    "\n",
    "# Original grid layout - THIS WILL NEVER BE MODIFIED\n",
    "ORIGINAL_GRID = [[\"0\" for _ in range(GRID_WIDTH)] for _ in range(GRID_HEIGHT)]\n",
    "\n",
    "# Place sample tiles\n",
    "ORIGINAL_GRID[18][1] = \"F\"    # Fireboy\n",
    "ORIGINAL_GRID[15][1] = \"W\"    # Watergirl\n",
    "ORIGINAL_GRID[2][19] = \"G\"    # Green agent\n",
    "\n",
    "# Poison tiles\n",
    "for x in range(10, 13):\n",
    "    ORIGINAL_GRID[18][x] = \"RP\"\n",
    "\n",
    "for x in range(17, 20):\n",
    "    ORIGINAL_GRID[18][x] = \"BP\"\n",
    "\n",
    "for x in range(18, 21):\n",
    "    ORIGINAL_GRID[14][x] = \"GP\"\n",
    "\n",
    "# Gems\n",
    "ORIGINAL_GRID[17][11] = ORIGINAL_GRID[8][10] = \"RB\"\n",
    "ORIGINAL_GRID[17][18] = ORIGINAL_GRID[10][20] = \"BB\"\n",
    "\n",
    "# Goals\n",
    "ORIGINAL_GRID[2][26] = \"FG\"\n",
    "ORIGINAL_GRID[2][28] = \"WG\"\n",
    "\n",
    "# Slides\n",
    "for x in range(1, 4):\n",
    "    ORIGINAL_GRID[9][x] = \"YS\"  # Yellow slides\n",
    "\n",
    "for x in range(26, 29):\n",
    "    ORIGINAL_GRID[6][x] = \"PS\"  # Purple slides\n",
    "\n",
    "# Buttons\n",
    "ORIGINAL_GRID[8][6] = ORIGINAL_GRID[12][9] = \"YB\"  # Yellow buttons\n",
    "ORIGINAL_GRID[8][14] = ORIGINAL_GRID[5][22] = \"PB\"  # Purple buttons\n",
    "\n",
    "# Assigning 1's for walls and platforms\n",
    "for i in range(0, 30):\n",
    "    ORIGINAL_GRID[0][i] = \"1\"\n",
    "    ORIGINAL_GRID[19][i] = \"1\"\n",
    "\n",
    "for i in range(0, 20):\n",
    "    ORIGINAL_GRID[i][0] = \"1\"\n",
    "    ORIGINAL_GRID[i][29] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[1][1] = ORIGINAL_GRID[1][2] = \"1\"\n",
    "for i in range(4, 29):\n",
    "    ORIGINAL_GRID[3][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[4][1] = ORIGINAL_GRID[4][2] = ORIGINAL_GRID[5][1] = ORIGINAL_GRID[5][2] = \"1\"\n",
    "\n",
    "for i in range(1, 26):\n",
    "    ORIGINAL_GRID[6][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[7][18] = ORIGINAL_GRID[7][19] = ORIGINAL_GRID[7][20] = \"1\"\n",
    "ORIGINAL_GRID[8][18] = ORIGINAL_GRID[8][19] = ORIGINAL_GRID[8][20] = \"1\"\n",
    "\n",
    "for i in range(4, 16):\n",
    "    ORIGINAL_GRID[9][i] = \"1\"\n",
    "\n",
    "for i in range(17, 22):\n",
    "    ORIGINAL_GRID[11][i] = \"1\"\n",
    "\n",
    "for i in range(22, 29):\n",
    "    ORIGINAL_GRID[10][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[10][16] = \"1\"\n",
    "ORIGINAL_GRID[11][26] = ORIGINAL_GRID[11][27] = ORIGINAL_GRID[11][28] = \"1\"\n",
    "ORIGINAL_GRID[12][26] = ORIGINAL_GRID[12][27] = ORIGINAL_GRID[12][28] = \"1\"\n",
    "\n",
    "for i in range(1, 13):\n",
    "    ORIGINAL_GRID[13][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[14][13] = \"1\"\n",
    "\n",
    "for i in range(14, 27):\n",
    "    ORIGINAL_GRID[15][i] = \"1\"\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ORIGINAL_GRID[16][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[17][27] = ORIGINAL_GRID[17][28] = ORIGINAL_GRID[18][27] = ORIGINAL_GRID[18][28] = \"1\"\n",
    "\n",
    "# Copy original grid to working grid\n",
    "grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "\n",
    "# Track slide positions and active buttons\n",
    "yellow_slides_positions = []\n",
    "purple_slides_positions = []\n",
    "yellow_buttons_active = False\n",
    "purple_buttons_active = False\n",
    "slide_animation_counter = 0\n",
    "SLIDE_ANIMATION_SPEED = 15  # Controls how fast slides move\n",
    "\n",
    "# Initialize slide positions\n",
    "def initialize_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions\n",
    "    yellow_slides_positions = []\n",
    "    purple_slides_positions = []\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if ORIGINAL_GRID[y][x] == \"YS\":\n",
    "                yellow_slides_positions.append((y, x))\n",
    "            elif ORIGINAL_GRID[y][x] == \"PS\":\n",
    "                purple_slides_positions.append((y, x))\n",
    "\n",
    "# Create grid maps for environment features\n",
    "def create_grid_map(feature_type):\n",
    "    \"\"\"Create a boolean grid map for different environmental features\"\"\"\n",
    "    grid_map = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=bool)\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            cell = grid[y][x]\n",
    "            if feature_type == 'wall' and cell == \"1\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_poison' and cell == \"RP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_poison' and cell == \"BP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'green_poison' and cell == \"GP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_gem' and cell == \"RB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_gem' and cell == \"BB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'fire_goal' and cell == \"FG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'water_goal' and cell == \"WG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_button' and cell == \"YB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_button' and cell == \"PB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_slide' and cell == \"YS\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_slide' and cell == \"PS\":\n",
    "                grid_map[y][x] = True\n",
    "    \n",
    "    return grid_map\n",
    "\n",
    "# Create all grid maps\n",
    "def initialize_environment_maps():\n",
    "    wall_map = create_grid_map('wall')\n",
    "    red_poison_map = create_grid_map('red_poison')\n",
    "    blue_poison_map = create_grid_map('blue_poison')\n",
    "    green_poison_map = create_grid_map('green_poison')\n",
    "    red_gem_map = create_grid_map('red_gem')\n",
    "    blue_gem_map = create_grid_map('blue_gem')\n",
    "    fire_goal_map = create_grid_map('fire_goal')\n",
    "    water_goal_map = create_grid_map('water_goal')\n",
    "    yellow_button_map = create_grid_map('yellow_button')\n",
    "    purple_button_map = create_grid_map('purple_button')\n",
    "    yellow_slide_map = create_grid_map('yellow_slide')\n",
    "    purple_slide_map = create_grid_map('purple_slide')\n",
    "    \n",
    "    # Create combined maps\n",
    "    poison_map = red_poison_map | blue_poison_map | green_poison_map\n",
    "    \n",
    "    # Create environment dictionary\n",
    "    environment = {\n",
    "        'wall': wall_map,\n",
    "        'poison': poison_map,\n",
    "        'red_poison': red_poison_map,\n",
    "        'blue_poison': blue_poison_map,\n",
    "        'green_poison': green_poison_map,\n",
    "        'red_gems': red_gem_map.copy(),\n",
    "        'blue_gems': blue_gem_map.copy(),\n",
    "        'fire_goal': fire_goal_map,\n",
    "        'water_goal': water_goal_map,\n",
    "        'yellow_button': yellow_button_map,\n",
    "        'purple_button': purple_button_map,\n",
    "        'yellow_slide': yellow_slide_map,\n",
    "        'purple_slide': purple_slide_map\n",
    "    }\n",
    "    \n",
    "    return environment\n",
    "\n",
    "# Agent position setup\n",
    "def get_initial_positions():\n",
    "    fireboy_pos = None\n",
    "    watergirl_pos = None\n",
    "    green_pos = None\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"F\":\n",
    "                fireboy_pos = (y, x)\n",
    "            elif grid[y][x] == \"W\":\n",
    "                watergirl_pos = (y, x)\n",
    "            elif grid[y][x] == \"G\":\n",
    "                green_pos = (y, x)\n",
    "    \n",
    "    return fireboy_pos, watergirl_pos, green_pos\n",
    "\n",
    "# Draw Function\n",
    "def draw_grid():\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            tile = grid[y][x]\n",
    "            color = COLORS.get(tile, BASE_COLOR)\n",
    "\n",
    "            # Agents: Heart shape\n",
    "            if tile in [\"F\", \"W\", \"G\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + 6),\n",
    "                    (x*TILE_SIZE + 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE - 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                ])\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + 2*TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "\n",
    "            # Gems: Rhombus\n",
    "            elif tile in [\"RB\", \"BB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2)\n",
    "                ])\n",
    "\n",
    "            # Buttons: Circle\n",
    "            elif tile in [\"YB\", \"PB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE//2), TILE_SIZE//4)\n",
    "\n",
    "            # Goals: Triangle\n",
    "            elif tile in [\"FG\", \"WG\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE)\n",
    "                ])\n",
    "\n",
    "            # Slides: Full tile fill\n",
    "            elif tile in [\"YS\", \"PS\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                \n",
    "            # Poison: Now blocks with specific colors\n",
    "            elif tile in [\"RP\", \"BP\", \"GP\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Default or walls\n",
    "            else:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Draw the black border\n",
    "            pygame.draw.rect(screen, (0, 0, 0), (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE), 1)\n",
    "\n",
    "# Enhanced neural network for MAPPO\n",
    "class MAPPONetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(MAPPONetwork, self).__init__()\n",
    "        # Actor network (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Return action probabilities and value\n",
    "        return F.softmax(self.actor(state), dim=-1), self.critic(state)\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        probs, _ = self.forward(state)\n",
    "        if deterministic:\n",
    "            return torch.argmax(probs).item()\n",
    "        else:\n",
    "            return torch.multinomial(probs, 1).item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        probs, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return log_prob, entropy, value\n",
    "\n",
    "# Define the DQN for the green agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values (not softmax)\n",
    "\n",
    "# Memory buffer for MAPPO\n",
    "class MAPPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.next_states.clear()\n",
    "        self.dones.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "# Experience replay memory for DQN\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, reward, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Initialize neural networks with proper hidden sizes\n",
    "HIDDEN_SIZE = 128\n",
    "INPUT_SIZE_MAPPO = 16  # Expanded input size for better state representation\n",
    "INPUT_SIZE_DQN = 24    # Even larger input size for the green agent\n",
    "\n",
    "# Initialize MAPPO networks for cooperative agents\n",
    "policy_f = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "policy_w = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "\n",
    "# Initialize DQN for the green adversarial agent\n",
    "dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "target_dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "target_dqn.load_state_dict(dqn.state_dict())  # Initially the same as the online network\n",
    "\n",
    "# Optimizers\n",
    "optimizer_f = optim.Adam(policy_f.parameters(), lr=0.0005)\n",
    "optimizer_w = optim.Adam(policy_w.parameters(), lr=0.0005)\n",
    "optimizer_dqn = optim.Adam(dqn.parameters(), lr=0.0005)\n",
    "\n",
    "# MAPPO memory buffers\n",
    "memory_f = MAPPOMemory()\n",
    "memory_w = MAPPOMemory()\n",
    "\n",
    "# DQN experience replay memory\n",
    "dqn_memory = ReplayMemory(10000)\n",
    "\n",
    "# DQN hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE = 10  # How often to update target network\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "# MAPPO hyperparameters\n",
    "PPO_EPOCHS = 4\n",
    "PPO_EPSILON = 0.2  # Clip parameter\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Initialize agent state\n",
    "def reset_environment():\n",
    "    # Reset grid to original state first\n",
    "    global grid, yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "    grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "    yellow_buttons_active = False\n",
    "    purple_buttons_active = False\n",
    "    slide_animation_counter = 0\n",
    "    \n",
    "    # Initialize slides\n",
    "    initialize_slides()\n",
    "    \n",
    "    # Get fresh positions from the reset grid\n",
    "    fireboy_pos, watergirl_pos, green_pos = get_initial_positions()\n",
    "    \n",
    "    # Create agent dictionaries with position and other properties\n",
    "    red_agent = {'pos': fireboy_pos, 'color': 'red', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    blue_agent = {'pos': watergirl_pos, 'color': 'blue', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    green_agent = {'pos': green_pos, 'color': 'green', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    \n",
    "    # Create fresh environment maps from the reset grid\n",
    "    environment = initialize_environment_maps()\n",
    "    \n",
    "    return red_agent, blue_agent, green_agent, environment\n",
    "\n",
    "# Handle slide movement\n",
    "def update_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions, grid, slide_animation_counter\n",
    "    \n",
    "    # Clear previous slide positions from grid\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"YS\" or grid[y][x] == \"PS\":\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Update yellow slides\n",
    "    new_yellow_positions = []\n",
    "    for y, x in yellow_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if yellow_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_yellow_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"YS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in yellow_slides_positions):\n",
    "                    current_pos = [pos for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_yellow_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((y, x))\n",
    "                grid[y][x] = \"YS\"\n",
    "    \n",
    "    # Update purple slides\n",
    "    new_purple_positions = []\n",
    "    for y, x in purple_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if purple_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_purple_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"PS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in purple_slides_positions):\n",
    "                    current_pos = [pos for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_purple_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((y, x))\n",
    "                grid[y][x] = \"PS\"\n",
    "    \n",
    "    # Update slide positions for next frame\n",
    "    yellow_slides_positions = new_yellow_positions\n",
    "    purple_slides_positions = new_purple_positions\n",
    "    \n",
    "    # Update animation counter\n",
    "    if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "        slide_animation_counter += 1\n",
    "\n",
    "# Enhanced state representation for MAPPO\n",
    "def get_mappo_state(agent, other_agent, green_agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    oy, ox = other_agent['pos']\n",
    "    gy, gx = green_agent['pos']\n",
    "    \n",
    "    # Find locations of goals\n",
    "    fire_goal_positions = np.where(environment['fire_goal'])\n",
    "    water_goal_positions = np.where(environment['water_goal'])\n",
    "    fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0] if len(fire_goal_positions[0]) > 0 else (0, 0)\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0] if len(water_goal_positions[0]) > 0 else (0, 0)\n",
    "    \n",
    "    # Target goal based on agent color\n",
    "    goal_y, goal_x = (fgy, fgx) if agent['color'] == 'red' else (wgy, wgx)\n",
    "    \n",
    "    # Calculate distances\n",
    "    dist_to_goal = np.sqrt((y - goal_y)**2 + (x - goal_x)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_other = np.sqrt((y - oy)**2 + (x - ox)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_green = np.sqrt((y - gy)**2 + (x - gx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Comprehensive state representation\n",
    "    state = [\n",
    "        y / GRID_HEIGHT,                  # Normalized y position\n",
    "        x / GRID_WIDTH,                   # Normalized x position\n",
    "        oy / GRID_HEIGHT,                 # Other agent y position\n",
    "        ox / GRID_WIDTH,                  # Other agent x position\n",
    "        gy / GRID_HEIGHT,                 # Green agent y position\n",
    "        gx / GRID_WIDTH,                  # Green agent x position\n",
    "        dist_to_goal,                     # Distance to own goal\n",
    "        dist_to_other,                    # Distance to other agent\n",
    "        dist_to_green,                    # Distance to green agent\n",
    "        1.0 if agent['is_pressing_button'] else 0.0,  # Is agent pressing button\n",
    "        1.0 if other_agent['is_pressing_button'] else 0.0,  # Is other agent pressing button\n",
    "        1.0 if yellow_buttons_active else 0.0,        # Is yellow button active\n",
    "        1.0 if purple_buttons_active else 0.0,        # Is purple button active\n",
    "        1.0 if agent['goal_reached'] else 0.0,        # Has agent reached goal\n",
    "        agent['collected_gems'] / 2.0,                # Normalized collected gems\n",
    "        other_agent['collected_gems'] / 2.0           # Other agent's collected gems\n",
    "    ]\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "# Enhanced state representation for DQN (green agent)\n",
    "def get_dqn_state(green_agent, red_agent, blue_agent, environment):\n",
    "    gy, gx = green_agent['pos']\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    \n",
    "    # Find locations of goals\n",
    "    fire_goal_positions = np.where(environment['fire_goal'])\n",
    "    water_goal_positions = np.where(environment['water_goal'])\n",
    "    fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0] if len(fire_goal_positions[0]) > 0 else (0, 0)\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0] if len(water_goal_positions[0]) > 0 else (0, 0)\n",
    "    \n",
    "    # Calculate distances to fire/water agents\n",
    "    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Calculate distances from fire/water to their goals\n",
    "    red_to_goal = np.sqrt((ry - fgy)**2 + (rx - fgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    blue_to_goal = np.sqrt((by - wgy)**2 + (bx - wgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Look in different directions for obstacles and agents\n",
    "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # right, down, left, up\n",
    "    dir_features = []\n",
    "    \n",
    "    for dy, dx in directions:\n",
    "        ny, nx = gy + dy, gx + dx\n",
    "        # Check if there's a wall\n",
    "        if 0 <= ny < GRID_HEIGHT and 0 <= nx < GRID_WIDTH and environment['wall'][ny][nx]:\n",
    "            dir_features.append(1.0)\n",
    "        else:\n",
    "            dir_features.append(0.0)\n",
    "            \n",
    "        # Check if there's a fire/water agent\n",
    "        if (ny, nx) == (ry, rx) or (ny, nx) == (by, bx):\n",
    "            dir_features.append(1.0)\n",
    "        else:\n",
    "            dir_features.append(0.0)\n",
    "    \n",
    "    # Comprehensive state for green agent\n",
    "    state = [\n",
    "        gy / GRID_HEIGHT,                 # Normalized y position\n",
    "        gx / GRID_WIDTH,                  # Normalized x position\n",
    "        ry / GRID_HEIGHT,                 # Red agent y position\n",
    "        rx / GRID_WIDTH,                  # Red agent x position\n",
    "        by / GRID_HEIGHT,                 # Blue agent y position\n",
    "        bx / GRID_WIDTH,                  # Blue agent x position\n",
    "        dist_to_red,                      # Distance to red agent\n",
    "        dist_to_blue,                     # Distance to blue agent\n",
    "        red_to_goal,                      # Red agent's distance to goal\n",
    "        blue_to_goal,                     # Blue agent's distance to goal\n",
    "        1.0 if red_agent['goal_reached'] else 0.0,   # Has red agent reached goal\n",
    "        1.0 if blue_agent['goal_reached'] else 0.0,  # Has blue agent reached goal\n",
    "        red_agent['collected_gems'] / 2.0,           # Red agent's collected gems\n",
    "        blue_agent['collected_gems'] / 2.0,          # Blue agent's collected gems\n",
    "        1.0 if green_agent['is_pressing_button'] else 0.0,  # Is green agent pressing button\n",
    "        1.0 if yellow_buttons_active else 0.0,        # Is yellow button active\n",
    "        1.0 if purple_buttons_active else 0.0,        # Is purple button active\n",
    "    ]\n",
    "    \n",
    "    # Add the directional features\n",
    "    state.extend(dir_features)\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "# Move the agent based on action\n",
    "def move_agent(agent, action, environment):\n",
    "    y, x = agent['pos']\n",
    "    \n",
    "    # Define actions: 0=up, 1=right, 2=down, 3=left\n",
    "    dy, dx = [(0, 0), (-1, 0), (0, 1), (1, 0), (0, -1)][action]\n",
    "    \n",
    "    # Check if the move would take us to a wall\n",
    "    new_y, new_x = y + dy, x + dx\n",
    "    \n",
    "    # Don't move if hitting a wall\n",
    "    if (new_y < 0 or new_y >= GRID_HEIGHT or new_x < 0 or new_x >= GRID_WIDTH or \n",
    "        environment['wall'][new_y][new_x]):\n",
    "        return (y, x)  # Stay in place\n",
    "    \n",
    "    return (new_y, new_x)\n",
    "\n",
    "# Check for interactions based on agent movement\n",
    "def check_interactions(agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    result = {\n",
    "        'consumed_gem': False,\n",
    "        'died': False,\n",
    "        'reached_goal': False,\n",
    "        'pressed_button': False\n",
    "    }\n",
    "    \n",
    "    # Check for gem collection\n",
    "    if agent['color'] == 'red' and environment['red_gems'][y][x]:\n",
    "        environment['red_gems'][y][x] = False\n",
    "        agent['collected_gems'] += 1\n",
    "        result['consumed_gem'] = True\n",
    "        # Remove the gem from the grid\n",
    "        if grid[y][x] == \"RB\":\n",
    "            grid[y][x] = \"0\"\n",
    "    \n",
    "    elif agent['color'] == 'blue' and environment['blue_gems'][y][x]:\n",
    "        environment['blue_gems'][y][x] = False\n",
    "        agent['collected_gems'] += 1\n",
    "        result['consumed_gem'] = True\n",
    "        # Remove the gem from the grid\n",
    "        if grid[y][x] == \"BB\":\n",
    "            grid[y][x] = \"0\"\n",
    "    \n",
    "    # Check for poison interaction\n",
    "    if (agent['color'] == 'red' and environment['blue_poison'][y][x]) or \\\n",
    "       (agent['color'] == 'blue' and environment['red_poison'][y][x]) or \\\n",
    "       (environment['green_poison'][y][x]):\n",
    "        agent['died'] = True\n",
    "        result['died'] = True\n",
    "    \n",
    "    # Check for goal reaching\n",
    "    if agent['color'] == 'red' and environment['fire_goal'][y][x]:\n",
    "        agent['goal_reached'] = True\n",
    "        result['reached_goal'] = True\n",
    "    elif agent['color'] == 'blue' and environment['water_goal'][y][x]:\n",
    "        agent['goal_reached'] = True\n",
    "        result['reached_goal'] = True\n",
    "    \n",
    "    # Check for button pressing\n",
    "    if environment['yellow_button'][y][x] or environment['purple_button'][y][x]:\n",
    "        agent['is_pressing_button'] = True\n",
    "        result['pressed_button'] = True\n",
    "        \n",
    "        # Activate corresponding button\n",
    "        global yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "        if environment['yellow_button'][y][x]:\n",
    "            yellow_buttons_active = True\n",
    "        if environment['purple_button'][y][x]:\n",
    "            purple_buttons_active = True\n",
    "        \n",
    "        # Reset animation counter when a button is pressed\n",
    "        slide_animation_counter = 0\n",
    "    else:\n",
    "        agent['is_pressing_button'] = False\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Update grid based on agent positions\n",
    "def update_grid_state(red_agent, blue_agent, green_agent):\n",
    "    # Clear previous agent positions\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] in [\"F\", \"W\", \"G\"]:\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Place agents in new positions\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    gy, gx = green_agent['pos']\n",
    "    \n",
    "    grid[ry][rx] = \"F\"\n",
    "    grid[by][bx] = \"W\"\n",
    "    grid[gy][gx] = \"G\"\n",
    "\n",
    "# Calculate rewards for fireboy and watergirl (cooperative)\n",
    "def calculate_cooperative_reward(agent, interactions, other_agent):\n",
    "    reward = 0\n",
    "    \n",
    "    # Base reward for survival\n",
    "    reward += 0.1\n",
    "    \n",
    "    # Reward for gem collection\n",
    "    if interactions['consumed_gem']:\n",
    "        reward += 1.0\n",
    "    \n",
    "    # Reward for reaching goal\n",
    "    if interactions['reached_goal']:\n",
    "        reward += 5.0\n",
    "    \n",
    "    # Penalty for death\n",
    "    if interactions['died']:\n",
    "        reward -= 5.0\n",
    "        \n",
    "    # Reward for button pressing\n",
    "    if interactions['pressed_button']:\n",
    "        reward += 0.5\n",
    "    \n",
    "    # Cooperative bonus when both agents reach their goals\n",
    "    if agent['goal_reached'] and other_agent['goal_reached']:\n",
    "        reward += 10.0\n",
    "    \n",
    "    # Penalty for being too far from other agent\n",
    "    y, x = agent['pos']\n",
    "    oy, ox = other_agent['pos']\n",
    "    distance = np.sqrt((y - oy)**2 + (x - ox)**2)\n",
    "    \n",
    "    # Encourage staying within a reasonable distance\n",
    "    if distance > 10:\n",
    "        reward -= 0.2\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Calculate rewards for green agent (adversarial)\n",
    "def calculate_adversarial_reward(green_agent, red_agent, blue_agent):\n",
    "    reward = 0\n",
    "    \n",
    "    # Base reward for survival\n",
    "    reward += 0.1\n",
    "    \n",
    "    # Reward for being close to fireboy/watergirl\n",
    "    gy, gx = green_agent['pos']\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    \n",
    "    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2)\n",
    "    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2)\n",
    "    \n",
    "    # Reward inversely proportional to distance to closest agent\n",
    "    min_dist = min(dist_to_red, dist_to_blue)\n",
    "    reward += max(0, (10 - min_dist) / 10) * 0.5\n",
    "    \n",
    "    # Reward for causing fireboy/watergirl to die\n",
    "    if red_agent['died'] or blue_agent['died']:\n",
    "        reward += 5.0\n",
    "    \n",
    "    # Penalty if both fireboy and watergirl reach their goals\n",
    "    if red_agent['goal_reached'] and blue_agent['goal_reached']:\n",
    "        reward -= 5.0\n",
    "    \n",
    "    # Reward for pressing buttons (interfering with fireboy/watergirl)\n",
    "    if green_agent['is_pressing_button']:\n",
    "        reward += 0.5\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# MAPPO training function\n",
    "def train_mappo(memory, policy, optimizer):\n",
    "    # Return early if not enough data\n",
    "    if len(memory) < 32:\n",
    "        return\n",
    "    \n",
    "    # Convert memory to tensors\n",
    "    states = torch.stack(memory.states)\n",
    "    actions = torch.tensor(memory.actions)\n",
    "    rewards = torch.tensor(memory.rewards)\n",
    "    next_states = torch.stack(memory.next_states)\n",
    "    dones = torch.tensor(memory.dones, dtype=torch.float)\n",
    "    old_log_probs = torch.stack(memory.log_probs)\n",
    "    old_values = torch.stack(memory.values)\n",
    "    \n",
    "    # Calculate returns and advantages\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    \n",
    "    # Get next state values\n",
    "    _, next_values = policy(next_states)\n",
    "    next_values = next_values.squeeze(-1)\n",
    "    \n",
    "    # Calculate returns and advantages\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        if i == len(rewards) - 1:\n",
    "            next_return = next_values[i] * (1 - dones[i])\n",
    "        else:\n",
    "            next_return = returns[0]\n",
    "        \n",
    "        # Calculate return\n",
    "        current_return = rewards[i] + GAMMA * next_return\n",
    "        returns.insert(0, current_return)\n",
    "        \n",
    "        # Calculate advantage using GAE\n",
    "        delta = rewards[i] + GAMMA * next_values[i] * (1 - dones[i]) - old_values[i]\n",
    "        gae = delta + GAMMA * GAE_LAMBDA * (1 - dones[i]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    returns = torch.tensor(returns)\n",
    "    advantages = torch.tensor(advantages)\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    # Perform PPO update\n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        # Get updated action probabilities and values\n",
    "        current_log_probs, entropies, current_values = [], [], []\n",
    "        \n",
    "        for i in range(len(states)):\n",
    "            log_prob, entropy, value = policy.evaluate(states[i], actions[i])\n",
    "            current_log_probs.append(log_prob)\n",
    "            entropies.append(entropy)\n",
    "            current_values.append(value)\n",
    "        \n",
    "        current_log_probs = torch.stack(current_log_probs)\n",
    "        entropies = torch.stack(entropies)\n",
    "        current_values = torch.stack(current_values).squeeze(-1)\n",
    "        \n",
    "        # Calculate ratios and surrogate losses\n",
    "        ratios = torch.exp(current_log_probs - old_log_probs.detach())\n",
    "        \n",
    "        # Calculate surrogate objectives\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1.0 - PPO_EPSILON, 1.0 + PPO_EPSILON) * advantages\n",
    "        \n",
    "        # Calculate actor, critic, and entropy losses\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = F.mse_loss(current_values, returns)\n",
    "        entropy_loss = -entropies.mean()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + VALUE_COEF * critic_loss + ENTROPY_COEF * entropy_loss\n",
    "        \n",
    "        # Perform optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Clear memory after update\n",
    "    memory.clear()\n",
    "\n",
    "# DQN training function\n",
    "def train_dqn():\n",
    "    global epsilon\n",
    "    \n",
    "    # Skip training if memory is too small\n",
    "    if len(dqn_memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    # Sample batch from memory\n",
    "    batch = dqn_memory.sample(BATCH_SIZE)\n",
    "    batch_states, batch_actions, batch_next_states, batch_rewards, batch_dones = zip(*batch)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states = torch.stack(batch_states)\n",
    "    actions = torch.tensor(batch_actions).unsqueeze(1)\n",
    "    next_states = torch.stack(batch_next_states)\n",
    "    rewards = torch.tensor(batch_rewards).unsqueeze(1)\n",
    "    dones = torch.tensor(batch_dones, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Compute Q values\n",
    "    q_values = dqn(states).gather(1, actions)\n",
    "    \n",
    "    # Compute next Q values with target network\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_dqn(next_states).max(1, keepdim=True)[0]\n",
    "    \n",
    "    # Compute target Q values\n",
    "    target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(q_values, target_q_values)\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer_dqn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_dqn.step()\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = max(EPSILON_END, EPSILON_START - (EPSILON_START - EPSILON_END) * (len(dqn_memory) / EPSILON_DECAY))\n",
    "\n",
    "# Fireboy action selection with MAPPO\n",
    "def select_fireboy_action(red_agent, blue_agent, green_agent, environment, training=True):\n",
    "    state = get_mappo_state(red_agent, blue_agent, green_agent, environment)\n",
    "    \n",
    "    if training:\n",
    "        probs, value = policy_f(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs[action])\n",
    "        return action, log_prob, value.item()\n",
    "    else:\n",
    "        return policy_f.get_action(state, deterministic=True), None, None\n",
    "\n",
    "# Watergirl action selection with MAPPO\n",
    "def select_watergirl_action(blue_agent, red_agent, green_agent, environment, training=True):\n",
    "    state = get_mappo_state(blue_agent, red_agent, green_agent, environment)\n",
    "    \n",
    "    if training:\n",
    "        probs, value = policy_w(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs[action])\n",
    "        return action, log_prob, value.item()\n",
    "    else:\n",
    "        return policy_w.get_action(state, deterministic=True), None, None\n",
    "\n",
    "# Green agent action selection with DQN\n",
    "def select_green_action(green_agent, red_agent, blue_agent, environment, training=True):\n",
    "    state = get_dqn_state(green_agent, red_agent, blue_agent, environment)\n",
    "    \n",
    "    if training and random.random() < epsilon:\n",
    "        return random.randint(0, NUM_ACTIONS - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(dqn(state)).item()\n",
    "\n",
    "# Main game loop\n",
    "def main():\n",
    "    global grid, epsilon, target_dqn\n",
    "    \n",
    "    # Training parameters\n",
    "    num_episodes = 10000\n",
    "    target_update_counter = 0\n",
    "    render_every = 100  # Only render every 100 episodes\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment\n",
    "        red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "        \n",
    "        # Track episode stats\n",
    "        episode_length = 0\n",
    "        episode_f_reward = 0\n",
    "        episode_w_reward = 0\n",
    "        episode_g_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Episode loop\n",
    "        while not done and episode_length < 200:  # Max 200 steps per episode\n",
    "            # Render if needed\n",
    "            if episode % render_every == 0:\n",
    "                # Handle pygame events\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        sys.exit()\n",
    "                \n",
    "                # Render the game\n",
    "                screen.fill((0, 0, 0))\n",
    "                draw_grid()\n",
    "                pygame.display.flip()\n",
    "                clock.tick(FPS)\n",
    "            \n",
    "            # Update slides and buttons\n",
    "            update_slides()\n",
    "            \n",
    "            # Fireboy's turn\n",
    "            f_action, f_log_prob, f_value = select_fireboy_action(red_agent, blue_agent, green_agent, environment)\n",
    "            red_agent['pos'] = move_agent(red_agent, f_action, environment)\n",
    "            \n",
    "            # Watergirl's turn\n",
    "            w_action, w_log_prob, w_value = select_watergirl_action(blue_agent, red_agent, green_agent, environment)\n",
    "            blue_agent['pos'] = move_agent(blue_agent, w_action, environment)\n",
    "            \n",
    "            # Green agent's turn\n",
    "            g_action = select_green_action(green_agent, red_agent, blue_agent, environment)\n",
    "            green_agent['pos'] = move_agent(green_agent, g_action, environment)\n",
    "            \n",
    "            # Update the grid\n",
    "            update_grid_state(red_agent, blue_agent, green_agent)\n",
    "            \n",
    "            # Check for interactions\n",
    "            f_interactions = check_interactions(red_agent, environment)\n",
    "            w_interactions = check_interactions(blue_agent, environment)\n",
    "            g_interactions = check_interactions(green_agent, environment)\n",
    "            \n",
    "            # Calculate rewards\n",
    "            f_reward = calculate_cooperative_reward(red_agent, f_interactions, blue_agent)\n",
    "            w_reward = calculate_cooperative_reward(blue_agent, w_interactions, red_agent)\n",
    "            g_reward = calculate_adversarial_reward(green_agent, red_agent, blue_agent)\n",
    "            \n",
    "            # Update episode rewards\n",
    "            episode_f_reward += f_reward\n",
    "            episode_w_reward += w_reward\n",
    "            episode_g_reward += g_reward\n",
    "            \n",
    "            # Get new states\n",
    "            f_next_state = get_mappo_state(red_agent, blue_agent, green_agent, environment)\n",
    "            w_next_state = get_mappo_state(blue_agent, red_agent, green_agent, environment)\n",
    "            g_next_state = get_dqn_state(green_agent, red_agent, blue_agent, environment)\n",
    "            \n",
    "            # Check if episode is done\n",
    "            done = (red_agent['died'] or blue_agent['died'] or\n",
    "                   (red_agent['goal_reached'] and blue_agent['goal_reached']))\n",
    "            \n",
    "            # Store experiences\n",
    "            memory_f.push(\n",
    "                get_mappo_state(red_agent, blue_agent, green_agent, environment),\n",
    "                f_action,\n",
    "                f_reward,\n",
    "                f_next_state,\n",
    "                done,\n",
    "                torch.tensor(f_log_prob),\n",
    "                torch.tensor(f_value)\n",
    "            )\n",
    "            \n",
    "            memory_w.push(\n",
    "                get_mappo_state(blue_agent, red_agent, green_agent, environment),\n",
    "                w_action,\n",
    "                w_reward,\n",
    "                w_next_state,\n",
    "                done,\n",
    "                torch.tensor(w_log_prob),\n",
    "                torch.tensor(w_value)\n",
    "            )\n",
    "            \n",
    "            dqn_memory.push(\n",
    "                get_dqn_state(green_agent, red_agent, blue_agent, environment),\n",
    "                g_action,\n",
    "                g_next_state,\n",
    "                g_reward,\n",
    "                done\n",
    "            )\n",
    "            \n",
    "            # Train models\n",
    "            train_mappo(memory_f, policy_f, optimizer_f)\n",
    "            train_mappo(memory_w, policy_w, optimizer_w)\n",
    "            train_dqn()\n",
    "            \n",
    "            episode_length += 1\n",
    "            \n",
    "            # Update target network\n",
    "            target_update_counter += 1\n",
    "            if target_update_counter % TARGET_UPDATE == 0:\n",
    "                target_dqn.load_state_dict(dqn.state_dict())\n",
    "        \n",
    "        # Print episode stats\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}\")\n",
    "            print(f\"F Reward: {episode_f_reward:.2f}, W Reward: {episode_w_reward:.2f}, G Reward: {episode_g_reward:.2f}\")\n",
    "            print(f\"Episode Length: {episode_length}\")\n",
    "            print(f\"Epsilon: {epsilon:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    # Save the trained models\n",
    "    torch.save(policy_f.state_dict(), \"policy_f.pth\")\n",
    "    torch.save(policy_w.state_dict(), \"policy_w.pth\")\n",
    "    torch.save(dqn.state_dict(), \"dqn.pth\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Run the game\n",
    "if __name__ == \"__main__\":\n",
    "    initialize_slides()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0557643-64d7-431c-b492-f0788402cd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205208b5-4d5e-4333-9d5c-453f89ef1c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'water_goal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1050\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1049\u001b[0m     initialize_slides()\n\u001b[1;32m-> 1050\u001b[0m     game_loop()\n",
      "Cell \u001b[1;32mIn[1], line 983\u001b[0m, in \u001b[0;36mgame_loop\u001b[1;34m()\u001b[0m\n\u001b[0;32m    981\u001b[0m red_state \u001b[38;5;241m=\u001b[39m get_mappo_state(red_agent, blue_agent, green_agent, environment)\n\u001b[0;32m    982\u001b[0m blue_state \u001b[38;5;241m=\u001b[39m get_mappo_state(blue_agent, red_agent, green_agent, environment)\n\u001b[1;32m--> 983\u001b[0m green_state \u001b[38;5;241m=\u001b[39m get_dqn_state(green_agent, red_agent, blue_agent, environment)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# Select actions\u001b[39;00m\n\u001b[0;32m    986\u001b[0m red_action, red_log_prob, red_value \u001b[38;5;241m=\u001b[39m select_mappo_action(policy_f, red_state)\n",
      "Cell \u001b[1;32mIn[1], line 602\u001b[0m, in \u001b[0;36mget_dqn_state\u001b[1;34m(green_agent, red_agent, blue_agent, environment)\u001b[0m\n\u001b[0;32m    600\u001b[0m water_goal_positions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(environment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwater_goal\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    601\u001b[0m fgy, fgx \u001b[38;5;241m=\u001b[39m fire_goal_positions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], fire_goal_positions[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fire_goal_positions[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 602\u001b[0m wgy, wgx \u001b[38;5;241m=\u001b[39m water_goal_positions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], water_goal\n\u001b[0;32m    603\u001b[0m wgy, wgx \u001b[38;5;241m=\u001b[39m water_goal_positions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], water_goal_positions[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(water_goal_positions[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# Distance calculations\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'water_goal' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "TILE_SIZE = 24\n",
    "GRID_WIDTH = 30\n",
    "GRID_HEIGHT = 20\n",
    "SCREEN_WIDTH = TILE_SIZE * GRID_WIDTH\n",
    "SCREEN_HEIGHT = TILE_SIZE * GRID_HEIGHT\n",
    "FPS = 10  # Increased FPS for smoother animation\n",
    "NUM_ACTIONS = 4  # Number of actions (up, down, left, right)\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
    "pygame.display.set_caption(\"Fireboy and Watergirl Grid World\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    \"0\": (120, 66, 18),   # Brown - Empty\n",
    "    \"1\": (160, 82, 45),   # Light Brown - Platform\n",
    "    \"F\": (255, 0, 0),     # Fireboy - Red Heart\n",
    "    \"W\": (0, 0, 255),     # Watergirl - Blue Heart\n",
    "    \"G\": (0, 255, 0),     # Green Agent\n",
    "    \"RP\": (255, 0, 0),  # Red Poison - Now a block\n",
    "    \"BP\": (0, 0, 255),  # Blue Poison - Now a block\n",
    "    \"GP\": (0, 255, 0),  # Green Poison - Now a block\n",
    "    \"FG\": (255, 0, 0),    # Fireboy Goal\n",
    "    \"WG\": (0, 0, 255),    # Watergirl Goal\n",
    "    \"RB\": (255, 0, 0),    # Red Gem\n",
    "    \"BB\": (0, 0, 255),    # Blue Gem\n",
    "    \"YS\": (255, 255, 0),  # Yellow Slide\n",
    "    \"PS\": (128, 0, 128),  # Purple Slide\n",
    "    \"YB\": (255, 255, 0),  # Yellow Button\n",
    "    \"PB\": (128, 0, 128),  # Purple Button\n",
    "}\n",
    "\n",
    "BASE_COLOR = COLORS[\"0\"]\n",
    "\n",
    "# Original grid layout - THIS WILL NEVER BE MODIFIED\n",
    "ORIGINAL_GRID = [[\"0\" for _ in range(GRID_WIDTH)] for _ in range(GRID_HEIGHT)]\n",
    "\n",
    "# Place sample tiles\n",
    "ORIGINAL_GRID[18][1] = \"F\"    # Fireboy\n",
    "ORIGINAL_GRID[15][1] = \"W\"    # Watergirl\n",
    "ORIGINAL_GRID[2][19] = \"G\"    # Green agent\n",
    "\n",
    "# Poison tiles\n",
    "for x in range(10, 13):\n",
    "    ORIGINAL_GRID[18][x] = \"RP\"\n",
    "\n",
    "for x in range(17, 20):\n",
    "    ORIGINAL_GRID[18][x] = \"BP\"\n",
    "\n",
    "for x in range(18, 21):\n",
    "    ORIGINAL_GRID[14][x] = \"GP\"\n",
    "\n",
    "# Gems\n",
    "ORIGINAL_GRID[17][11] = ORIGINAL_GRID[8][10] = \"RB\"\n",
    "ORIGINAL_GRID[17][18] = ORIGINAL_GRID[10][20] = \"BB\"\n",
    "\n",
    "# Goals\n",
    "ORIGINAL_GRID[2][26] = \"FG\"\n",
    "ORIGINAL_GRID[2][28] = \"WG\"\n",
    "\n",
    "# Slides\n",
    "for x in range(1, 4):\n",
    "    ORIGINAL_GRID[9][x] = \"YS\"  # Yellow slides\n",
    "\n",
    "for x in range(26, 29):\n",
    "    ORIGINAL_GRID[6][x] = \"PS\"  # Purple slides\n",
    "\n",
    "# Buttons\n",
    "ORIGINAL_GRID[8][6] = ORIGINAL_GRID[12][9] = \"YB\"  # Yellow buttons\n",
    "ORIGINAL_GRID[8][14] = ORIGINAL_GRID[5][22] = \"PB\"  # Purple buttons\n",
    "\n",
    "# Assigning 1's for walls and platforms\n",
    "for i in range(0, 30):\n",
    "    ORIGINAL_GRID[0][i] = \"1\"\n",
    "    ORIGINAL_GRID[19][i] = \"1\"\n",
    "\n",
    "for i in range(0, 20):\n",
    "    ORIGINAL_GRID[i][0] = \"1\"\n",
    "    ORIGINAL_GRID[i][29] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[1][1] = ORIGINAL_GRID[1][2] = \"1\"\n",
    "for i in range(4, 29):\n",
    "    ORIGINAL_GRID[3][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[4][1] = ORIGINAL_GRID[4][2] = ORIGINAL_GRID[5][1] = ORIGINAL_GRID[5][2] = \"1\"\n",
    "\n",
    "for i in range(1, 26):\n",
    "    ORIGINAL_GRID[6][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[7][18] = ORIGINAL_GRID[7][19] = ORIGINAL_GRID[7][20] = \"1\"\n",
    "ORIGINAL_GRID[8][18] = ORIGINAL_GRID[8][19] = ORIGINAL_GRID[8][20] = \"1\"\n",
    "\n",
    "for i in range(4, 16):\n",
    "    ORIGINAL_GRID[9][i] = \"1\"\n",
    "\n",
    "for i in range(17, 22):\n",
    "    ORIGINAL_GRID[11][i] = \"1\"\n",
    "\n",
    "for i in range(22, 29):\n",
    "    ORIGINAL_GRID[10][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[10][16] = \"1\"\n",
    "ORIGINAL_GRID[11][26] = ORIGINAL_GRID[11][27] = ORIGINAL_GRID[11][28] = \"1\"\n",
    "ORIGINAL_GRID[12][26] = ORIGINAL_GRID[12][27] = ORIGINAL_GRID[12][28] = \"1\"\n",
    "\n",
    "for i in range(1, 13):\n",
    "    ORIGINAL_GRID[13][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[14][13] = \"1\"\n",
    "\n",
    "for i in range(14, 27):\n",
    "    ORIGINAL_GRID[15][i] = \"1\"\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ORIGINAL_GRID[16][i] = \"1\"\n",
    "\n",
    "ORIGINAL_GRID[17][27] = ORIGINAL_GRID[17][28] = ORIGINAL_GRID[18][27] = ORIGINAL_GRID[18][28] = \"1\"\n",
    "\n",
    "# Copy original grid to working grid\n",
    "grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "\n",
    "# Track slide positions and active buttons\n",
    "yellow_slides_positions = []\n",
    "purple_slides_positions = []\n",
    "yellow_buttons_active = False\n",
    "purple_buttons_active = False\n",
    "slide_animation_counter = 0\n",
    "SLIDE_ANIMATION_SPEED = 15  # Controls how fast slides move\n",
    "\n",
    "# Initialize slide positions\n",
    "def initialize_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions\n",
    "    yellow_slides_positions = []\n",
    "    purple_slides_positions = []\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if ORIGINAL_GRID[y][x] == \"YS\":\n",
    "                yellow_slides_positions.append((y, x))\n",
    "            elif ORIGINAL_GRID[y][x] == \"PS\":\n",
    "                purple_slides_positions.append((y, x))\n",
    "\n",
    "# Create grid maps for environment features\n",
    "def create_grid_map(feature_type):\n",
    "    \"\"\"Create a boolean grid map for different environmental features\"\"\"\n",
    "    grid_map = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=bool)\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            cell = grid[y][x]\n",
    "            if feature_type == 'wall' and cell == \"1\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_poison' and cell == \"RP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_poison' and cell == \"BP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'green_poison' and cell == \"GP\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'red_gem' and cell == \"RB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'blue_gem' and cell == \"BB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'fire_goal' and cell == \"FG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'water_goal' and cell == \"WG\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_button' and cell == \"YB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_button' and cell == \"PB\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'yellow_slide' and cell == \"YS\":\n",
    "                grid_map[y][x] = True\n",
    "            elif feature_type == 'purple_slide' and cell == \"PS\":\n",
    "                grid_map[y][x] = True\n",
    "    \n",
    "    return grid_map\n",
    "\n",
    "# Create all grid maps\n",
    "def initialize_environment_maps():\n",
    "    wall_map = create_grid_map('wall')\n",
    "    red_poison_map = create_grid_map('red_poison')\n",
    "    blue_poison_map = create_grid_map('blue_poison')\n",
    "    green_poison_map = create_grid_map('green_poison')\n",
    "    red_gem_map = create_grid_map('red_gem')\n",
    "    blue_gem_map = create_grid_map('blue_gem')\n",
    "    fire_goal_map = create_grid_map('fire_goal')\n",
    "    water_goal_map = create_grid_map('water_goal')\n",
    "    yellow_button_map = create_grid_map('yellow_button')\n",
    "    purple_button_map = create_grid_map('purple_button')\n",
    "    yellow_slide_map = create_grid_map('yellow_slide')\n",
    "    purple_slide_map = create_grid_map('purple_slide')\n",
    "    \n",
    "    # Create combined maps\n",
    "    poison_map = red_poison_map | blue_poison_map | green_poison_map\n",
    "    \n",
    "    # Create environment dictionary\n",
    "    environment = {\n",
    "        'wall': wall_map,\n",
    "        'poison': poison_map,\n",
    "        'red_poison': red_poison_map,\n",
    "        'blue_poison': blue_poison_map,\n",
    "        'green_poison': green_poison_map,\n",
    "        'red_gems': red_gem_map.copy(),\n",
    "        'blue_gems': blue_gem_map.copy(),\n",
    "        'fire_goal': fire_goal_map,\n",
    "        'water_goal': water_goal_map,\n",
    "        'yellow_button': yellow_button_map,\n",
    "        'purple_button': purple_button_map,\n",
    "        'yellow_slide': yellow_slide_map,\n",
    "        'purple_slide': purple_slide_map\n",
    "    }\n",
    "    \n",
    "    return environment\n",
    "\n",
    "# Agent position setup\n",
    "def get_initial_positions():\n",
    "    fireboy_pos = None\n",
    "    watergirl_pos = None\n",
    "    green_pos = None\n",
    "    \n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"F\":\n",
    "                fireboy_pos = (y, x)\n",
    "            elif grid[y][x] == \"W\":\n",
    "                watergirl_pos = (y, x)\n",
    "            elif grid[y][x] == \"G\":\n",
    "                green_pos = (y, x)\n",
    "    \n",
    "    return fireboy_pos, watergirl_pos, green_pos\n",
    "\n",
    "# Draw Function\n",
    "def draw_grid():\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            tile = grid[y][x]\n",
    "            color = COLORS.get(tile, BASE_COLOR)\n",
    "\n",
    "            # Agents: Heart shape\n",
    "            if tile in [\"F\", \"W\", \"G\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + 6),\n",
    "                    (x*TILE_SIZE + 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE - 6, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                ])\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + 2*TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)\n",
    "\n",
    "            # Gems: Rhombus\n",
    "            elif tile in [\"RB\", \"BB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2),\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2)\n",
    "                ])\n",
    "\n",
    "            # Buttons: Circle\n",
    "            elif tile in [\"YB\", \"PB\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE//2), TILE_SIZE//4)\n",
    "\n",
    "            # Goals: Triangle\n",
    "            elif tile in [\"FG\", \"WG\"]:\n",
    "                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                pygame.draw.polygon(screen, color, [\n",
    "                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),\n",
    "                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE),\n",
    "                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE)\n",
    "                ])\n",
    "\n",
    "            # Slides: Full tile fill\n",
    "            elif tile in [\"YS\", \"PS\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "                \n",
    "            # Poison: Now blocks with specific colors\n",
    "            elif tile in [\"RP\", \"BP\", \"GP\"]:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Default or walls\n",
    "            else:\n",
    "                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "            # Draw the black border\n",
    "            pygame.draw.rect(screen, (0, 0, 0), (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE), 1)\n",
    "\n",
    "# Enhanced neural network for MAPPO\n",
    "class MAPPONetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, action_size):\n",
    "        super(MAPPONetwork, self).__init__()\n",
    "        # Actor network (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "        )\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Return action probabilities and value\n",
    "        return F.softmax(self.actor(state), dim=-1), self.critic(state)\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        probs, _ = self.forward(state)\n",
    "        if deterministic:\n",
    "            return torch.argmax(probs).item()\n",
    "        else:\n",
    "            return torch.multinomial(probs, 1).item()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        probs, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return log_prob, entropy, value\n",
    "\n",
    "# Define the DQN for the green agent\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values (not softmax)\n",
    "\n",
    "# Memory buffer for MAPPO\n",
    "class MAPPOMemory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.dones.append(done)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "        self.next_states.clear()\n",
    "        self.dones.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.values.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "# Experience replay memory for DQN\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, reward, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Initialize neural networks with proper hidden sizes\n",
    "HIDDEN_SIZE = 128\n",
    "INPUT_SIZE_MAPPO = 16  # Expanded input size for better state representation\n",
    "# FIX: Match the input size to the actual features in the state\n",
    "INPUT_SIZE_DQN = 25    # Updated to match actual state size from get_dqn_state\n",
    "\n",
    "# Initialize MAPPO networks for cooperative agents\n",
    "policy_f = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "policy_w = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "\n",
    "# Initialize DQN for the green adversarial agent\n",
    "dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "target_dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)\n",
    "target_dqn.load_state_dict(dqn.state_dict())  # Initially the same as the online network\n",
    "\n",
    "# Optimizers\n",
    "optimizer_f = optim.Adam(policy_f.parameters(), lr=0.0005)\n",
    "optimizer_w = optim.Adam(policy_w.parameters(), lr=0.0005)\n",
    "optimizer_dqn = optim.Adam(dqn.parameters(), lr=0.0005)\n",
    "\n",
    "# MAPPO memory buffers\n",
    "memory_f = MAPPOMemory()\n",
    "memory_w = MAPPOMemory()\n",
    "\n",
    "# DQN experience replay memory\n",
    "dqn_memory = ReplayMemory(10000)\n",
    "\n",
    "# DQN hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE = 10  # How often to update target network\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "# MAPPO hyperparameters\n",
    "PPO_EPOCHS = 4\n",
    "PPO_EPSILON = 0.2  # Clip parameter\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "GAE_LAMBDA = 0.95\n",
    "\n",
    "# Initialize agent state\n",
    "def reset_environment():\n",
    "    # Reset grid to original state first\n",
    "    global grid, yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "    grid = [[cell for cell in row] for row in ORIGINAL_GRID]\n",
    "    yellow_buttons_active = False\n",
    "    purple_buttons_active = False\n",
    "    slide_animation_counter = 0\n",
    "    \n",
    "    # Initialize slides\n",
    "    initialize_slides()\n",
    "    \n",
    "    # Get fresh positions from the reset grid\n",
    "    fireboy_pos, watergirl_pos, green_pos = get_initial_positions()\n",
    "    \n",
    "    # Create agent dictionaries with position and other properties\n",
    "    red_agent = {'pos': fireboy_pos, 'color': 'red', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    blue_agent = {'pos': watergirl_pos, 'color': 'blue', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    green_agent = {'pos': green_pos, 'color': 'green', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}\n",
    "    \n",
    "    # Create fresh environment maps from the reset grid\n",
    "    environment = initialize_environment_maps()\n",
    "    \n",
    "    return red_agent, blue_agent, green_agent, environment\n",
    "\n",
    "# Handle slide movement\n",
    "def update_slides():\n",
    "    global yellow_slides_positions, purple_slides_positions, grid, slide_animation_counter\n",
    "    \n",
    "    # Clear previous slide positions from grid\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] == \"YS\" or grid[y][x] == \"PS\":\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Update yellow slides\n",
    "    new_yellow_positions = []\n",
    "    for y, x in yellow_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if yellow_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_yellow_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"YS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in yellow_slides_positions):\n",
    "                    current_pos = [pos for pos in yellow_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_yellow_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"YS\"\n",
    "            else:\n",
    "                new_yellow_positions.append((y, x))\n",
    "                grid[y][x] = \"YS\"\n",
    "    \n",
    "    # Update purple slides\n",
    "    new_purple_positions = []\n",
    "    for y, x in purple_slides_positions:\n",
    "        # If button is pressed, move down 3 rows, otherwise stay in original position\n",
    "        if purple_buttons_active:\n",
    "            target_y = min(y + 3, GRID_HEIGHT - 1)\n",
    "            # Check if we're in animation\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                # Calculate intermediate position\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                current_y = int(y + (target_y - y) * animation_progress)\n",
    "                new_purple_positions.append((current_y, x))\n",
    "                grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((target_y, x))\n",
    "                grid[target_y][x] = \"PS\"\n",
    "        else:\n",
    "            # If buttons not active, return to original position\n",
    "            if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "                original_y = [pos[0] for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED\n",
    "                # Find the current position in the animation\n",
    "                if any(pos[1] == x for pos in purple_slides_positions):\n",
    "                    current_pos = [pos for pos in purple_slides_positions if pos[1] == x][0]\n",
    "                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)\n",
    "                    new_purple_positions.append((current_y, x))\n",
    "                    grid[current_y][x] = \"PS\"\n",
    "            else:\n",
    "                new_purple_positions.append((y, x))\n",
    "                grid[y][x] = \"PS\"\n",
    "    \n",
    "    # Update slide positions for next frame\n",
    "    yellow_slides_positions = new_yellow_positions\n",
    "    purple_slides_positions = new_purple_positions\n",
    "    \n",
    "    # Update animation counter\n",
    "    if slide_animation_counter < SLIDE_ANIMATION_SPEED:\n",
    "        slide_animation_counter += 1\n",
    "\n",
    "# Enhanced state representation for MAPPO\n",
    "def get_mappo_state(agent, other_agent, green_agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    oy, ox = other_agent['pos']\n",
    "    gy, gx = green_agent['pos']\n",
    "    \n",
    "    # Find locations of goals\n",
    "    fire_goal_positions = np.where(environment['fire_goal'])\n",
    "    water_goal_positions = np.where(environment['water_goal'])\n",
    "    fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0] if len(fire_goal_positions[0]) > 0 else (0, 0)\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0] if len(water_goal_positions[0]) > 0 else (0, 0)\n",
    "    \n",
    "    # Target goal based on agent color\n",
    "    goal_y, goal_x = (fgy, fgx) if agent['color'] == 'red' else (wgy, wgx)\n",
    "    \n",
    "    # Calculate distances\n",
    "    dist_to_goal = np.sqrt((y - goal_y)**2 + (x - goal_x)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_other = np.sqrt((y - oy)**2 + (x - ox)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_green = np.sqrt((y - gy)**2 + (x - gx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Comprehensive state representation\n",
    "    state = [\n",
    "        y / GRID_HEIGHT,                  # Normalized y position\n",
    "        x / GRID_WIDTH,                   # Normalized x position\n",
    "        oy / GRID_HEIGHT,                 # Other agent y position\n",
    "        ox / GRID_WIDTH,                  # Other agent x position\n",
    "        gy / GRID_HEIGHT,                 # Green agent y position\n",
    "        gx / GRID_WIDTH,                  # Green agent x position\n",
    "        dist_to_goal,                     # Distance to own goal\n",
    "        dist_to_other,                    # Distance to other agent\n",
    "        dist_to_green,                    # Distance to green agent\n",
    "        1.0 if agent['is_pressing_button'] else 0.0,  # Is agent pressing button\n",
    "        1.0 if other_agent['is_pressing_button'] else 0.0,  # Is other agent pressing button\n",
    "        1.0 if yellow_buttons_active else 0.0,        # Is yellow button active\n",
    "        1.0 if purple_buttons_active else 0.0,        # Is purple button active\n",
    "        1.0 if agent['goal_reached'] else 0.0,        # Has agent reached goal\n",
    "        agent['collected_gems'] / 2.0,                # Normalized collected gems\n",
    "        other_agent['collected_gems'] / 2.0           # Other agent's collected gems\n",
    "    ]\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "# Enhanced state representation for DQN (green agent)\n",
    "def get_dqn_state(green_agent, red_agent, blue_agent, environment):\n",
    "    gy, gx = green_agent['pos']\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    \n",
    "    # Find locations of goals\n",
    "    fire_goal_positions = np.where(environment['fire_goal'])\n",
    "    water_goal_positions = np.where(environment['water_goal'])\n",
    "    fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0] if len(fire_goal_positions[0]) > 0 else (0, 0)\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal\n",
    "    wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0] if len(water_goal_positions[0]) > 0 else (0, 0)\n",
    "    \n",
    "    # Distance calculations\n",
    "    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_fire_goal = np.sqrt((gy - fgy)**2 + (gx - fgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    dist_to_water_goal = np.sqrt((gy - wgy)**2 + (gx - wgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)\n",
    "    \n",
    "    # Get button information\n",
    "    yellow_button_positions = np.where(environment['yellow_button'])\n",
    "    purple_button_positions = np.where(environment['purple_button'])\n",
    "    \n",
    "    # Find distances to buttons\n",
    "    dist_to_yellow_button = min([np.sqrt((gy - y)**2 + (gx - x)**2) for y, x in zip(yellow_button_positions[0], yellow_button_positions[1])]) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2) if len(yellow_button_positions[0]) > 0 else 1.0\n",
    "    dist_to_purple_button = min([np.sqrt((gy - y)**2 + (gx - x)**2) for y, x in zip(purple_button_positions[0], purple_button_positions[1])]) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2) if len(purple_button_positions[0]) > 0 else 1.0\n",
    "    \n",
    "    # Check surroundings (4 adjacent cells)\n",
    "    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "    surroundings = []\n",
    "    for dy, dx in directions:\n",
    "        ny, nx = gy + dy, gx + dx\n",
    "        if 0 <= ny < GRID_HEIGHT and 0 <= nx < GRID_WIDTH:\n",
    "            if environment['wall'][ny][nx]:\n",
    "                surroundings.append(1.0)  # Wall\n",
    "            elif (ny, nx) == (ry, rx):\n",
    "                surroundings.append(2.0)  # Red agent\n",
    "            elif (ny, nx) == (by, bx):\n",
    "                surroundings.append(3.0)  # Blue agent\n",
    "            else:\n",
    "                surroundings.append(0.0)  # Empty\n",
    "        else:\n",
    "            surroundings.append(1.0)  # Out of bounds (treat as wall)\n",
    "    \n",
    "    # Comprehensive state representation for the green agent\n",
    "    state = [\n",
    "        gy / GRID_HEIGHT,                  # Normalized y position\n",
    "        gx / GRID_WIDTH,                   # Normalized x position\n",
    "        ry / GRID_HEIGHT,                  # Red agent y position\n",
    "        rx / GRID_WIDTH,                   # Red agent x position\n",
    "        by / GRID_HEIGHT,                  # Blue agent y position\n",
    "        bx / GRID_WIDTH,                   # Blue agent x position\n",
    "        dist_to_red,                       # Distance to red agent\n",
    "        dist_to_blue,                      # Distance to blue agent\n",
    "        dist_to_fire_goal,                 # Distance to fire goal\n",
    "        dist_to_water_goal,                # Distance to water goal\n",
    "        dist_to_yellow_button,             # Distance to closest yellow button\n",
    "        dist_to_purple_button,             # Distance to closest purple button\n",
    "        1.0 if yellow_buttons_active else 0.0,  # Is yellow button active\n",
    "        1.0 if purple_buttons_active else 0.0,  # Is purple button active\n",
    "        1.0 if green_agent['is_pressing_button'] else 0.0,  # Is green agent pressing button\n",
    "        1.0 if red_agent['goal_reached'] else 0.0,  # Has red agent reached goal\n",
    "        1.0 if blue_agent['goal_reached'] else 0.0,  # Has blue agent reached goal\n",
    "        red_agent['collected_gems'] / 2.0,  # Red agent's collected gems\n",
    "        blue_agent['collected_gems'] / 2.0,  # Blue agent's collected gems\n",
    "    ] + surroundings  # Add surroundings (4 values)\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "# Get actions and update agent positions\n",
    "def step_environment(red_agent, blue_agent, green_agent, environment, red_action, blue_action, green_action):\n",
    "    # Process button presses first (from previous state)\n",
    "    check_button_presses(red_agent, blue_agent, green_agent, environment)\n",
    "    \n",
    "    # Update slide positions based on button activity\n",
    "    update_slides()\n",
    "    \n",
    "    # Update environment maps after potential slide movement\n",
    "    environment = initialize_environment_maps()\n",
    "    \n",
    "    # Process agent movements based on actions\n",
    "    red_done = move_agent(red_agent, environment, red_action)\n",
    "    blue_done = move_agent(blue_agent, environment, blue_action)\n",
    "    green_done = move_agent(green_agent, environment, green_action)\n",
    "    \n",
    "    # Check collectibles and hazards after movement\n",
    "    red_reward = check_collectibles_and_hazards(red_agent, environment)\n",
    "    blue_reward = check_collectibles_and_hazards(blue_agent, environment)\n",
    "    green_reward = check_collectibles_and_hazards(green_agent, environment)\n",
    "    \n",
    "    # Check if agents reached their goals\n",
    "    red_reached_goal = check_goal(red_agent, environment, 'fire_goal')\n",
    "    blue_reached_goal = check_goal(blue_agent, environment, 'water_goal')\n",
    "    \n",
    "    # Calculate reward based on game state\n",
    "    # For cooperative agents (red and blue)\n",
    "    if red_agent['died'] or blue_agent['died']:\n",
    "        red_reward -= 10\n",
    "        blue_reward -= 10\n",
    "    \n",
    "    if red_reached_goal:\n",
    "        red_agent['goal_reached'] = True\n",
    "        red_reward += 20\n",
    "        blue_reward += 10\n",
    "    \n",
    "    if blue_reached_goal:\n",
    "        blue_agent['goal_reached'] = True\n",
    "        blue_reward += 20\n",
    "        red_reward += 10\n",
    "    \n",
    "    # Both reached goal is a big win\n",
    "    if red_agent['goal_reached'] and blue_agent['goal_reached']:\n",
    "        red_reward += 50\n",
    "        blue_reward += 50\n",
    "    \n",
    "    # For adversarial agent (green)\n",
    "    if red_agent['died'] or blue_agent['died']:\n",
    "        green_reward += 15\n",
    "    \n",
    "    if red_agent['goal_reached'] or blue_agent['goal_reached']:\n",
    "        green_reward -= 10\n",
    "    \n",
    "    if red_agent['goal_reached'] and blue_agent['goal_reached']:\n",
    "        green_reward -= 30\n",
    "    \n",
    "    # Check if episode is done\n",
    "    done = (red_agent['died'] or blue_agent['died'] or \n",
    "            (red_agent['goal_reached'] and blue_agent['goal_reached']) or\n",
    "            red_done or blue_done or green_done)\n",
    "    \n",
    "    # Update grid representation for visualization\n",
    "    update_grid_representation(red_agent, blue_agent, green_agent)\n",
    "    \n",
    "    return red_reward, blue_reward, green_reward, done, environment\n",
    "\n",
    "# Check if agent is on a button\n",
    "def check_button_presses(red_agent, blue_agent, green_agent, environment):\n",
    "    global yellow_buttons_active, purple_buttons_active, slide_animation_counter\n",
    "    \n",
    "    # Reset button states\n",
    "    yellow_buttons_pressed = False\n",
    "    purple_buttons_pressed = False\n",
    "    \n",
    "    # Check each agent\n",
    "    for agent in [red_agent, blue_agent, green_agent]:\n",
    "        y, x = agent['pos']\n",
    "        if environment['yellow_button'][y][x]:\n",
    "            yellow_buttons_pressed = True\n",
    "            agent['is_pressing_button'] = True\n",
    "        elif environment['purple_button'][y][x]:\n",
    "            purple_buttons_pressed = True\n",
    "            agent['is_pressing_button'] = True\n",
    "        else:\n",
    "            agent['is_pressing_button'] = False\n",
    "    \n",
    "    # If button state changed, reset animation counter\n",
    "    if yellow_buttons_pressed != yellow_buttons_active or purple_buttons_pressed != purple_buttons_active:\n",
    "        slide_animation_counter = 0\n",
    "    \n",
    "    yellow_buttons_active = yellow_buttons_pressed\n",
    "    purple_buttons_active = purple_buttons_pressed\n",
    "\n",
    "# Move agent based on action\n",
    "def move_agent(agent, environment, action):\n",
    "    y, x = agent['pos']\n",
    "    \n",
    "    if action == 0:  # UP\n",
    "        y -= 1\n",
    "    elif action == 1:  # RIGHT\n",
    "        x += 1\n",
    "    elif action == 2:  # DOWN\n",
    "        y += 1\n",
    "    elif action == 3:  # LEFT\n",
    "        x -= 1\n",
    "    \n",
    "    # Check if new position is valid\n",
    "    if 0 <= y < GRID_HEIGHT and 0 <= x < GRID_WIDTH and not environment['wall'][y][x]:\n",
    "        agent['pos'] = (y, x)\n",
    "    \n",
    "    # Check if agent is out of bounds or in an invalid position\n",
    "    return not (0 <= y < GRID_HEIGHT and 0 <= x < GRID_WIDTH)\n",
    "\n",
    "# Check for collectibles and hazards\n",
    "def check_collectibles_and_hazards(agent, environment):\n",
    "    y, x = agent['pos']\n",
    "    reward = 0\n",
    "    \n",
    "    # Check for gems\n",
    "    if agent['color'] == 'red' and environment['red_gems'][y][x]:\n",
    "        environment['red_gems'][y][x] = False\n",
    "        agent['collected_gems'] += 1\n",
    "        reward += 5\n",
    "        # Clear gem from grid for visualization\n",
    "        grid[y][x] = \"0\"\n",
    "    \n",
    "    elif agent['color'] == 'blue' and environment['blue_gems'][y][x]:\n",
    "        environment['blue_gems'][y][x] = False\n",
    "        agent['collected_gems'] += 1\n",
    "        reward += 5\n",
    "        # Clear gem from grid for visualization\n",
    "        grid[y][x] = \"0\"\n",
    "    \n",
    "    # Check for poison (specific to agent type)\n",
    "    if (agent['color'] == 'red' and environment['blue_poison'][y][x]) or \\\n",
    "       (agent['color'] == 'blue' and environment['red_poison'][y][x]) or \\\n",
    "       (agent['color'] in ['red', 'blue', 'green'] and environment['green_poison'][y][x]):\n",
    "        agent['died'] = True\n",
    "        reward -= 20\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Check if agent reached goal\n",
    "def check_goal(agent, environment, goal_type):\n",
    "    y, x = agent['pos']\n",
    "    if environment[goal_type][y][x]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Update grid representation for visualization\n",
    "def update_grid_representation(red_agent, blue_agent, green_agent):\n",
    "    # Clear agent positions from grid\n",
    "    for y in range(GRID_HEIGHT):\n",
    "        for x in range(GRID_WIDTH):\n",
    "            if grid[y][x] in [\"F\", \"W\", \"G\"]:\n",
    "                grid[y][x] = \"0\"\n",
    "    \n",
    "    # Add agents to grid\n",
    "    ry, rx = red_agent['pos']\n",
    "    by, bx = blue_agent['pos']\n",
    "    gy, gx = green_agent['pos']\n",
    "    \n",
    "    grid[ry][rx] = \"F\"\n",
    "    grid[by][bx] = \"W\"\n",
    "    grid[gy][gx] = \"G\"\n",
    "\n",
    "# Select action using MAPPO policies (with exploration)\n",
    "def select_mappo_action(policy, state, deterministic=False):\n",
    "    with torch.no_grad():\n",
    "        action_probs, value = policy(state)\n",
    "        if deterministic:\n",
    "            action = torch.argmax(action_probs).item()\n",
    "        else:\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "        log_prob = torch.log(action_probs[action])\n",
    "        \n",
    "    return action, log_prob.item(), value.item()\n",
    "\n",
    "# Select action using DQN (with epsilon-greedy exploration)\n",
    "def select_dqn_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, NUM_ACTIONS - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(state)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "# Training functions\n",
    "def train_mappo(memories, policies, optimizers, gamma, epsilon_clip, value_coef, entropy_coef):\n",
    "    for memory, policy, optimizer in zip(memories, policies, optimizers):\n",
    "        if len(memory) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        states = torch.stack(memory.states)\n",
    "        actions = torch.tensor(memory.actions)\n",
    "        rewards = torch.tensor(memory.rewards)\n",
    "        next_states = torch.stack(memory.next_states)\n",
    "        dones = torch.tensor(memory.dones).float()\n",
    "        old_log_probs = torch.tensor(memory.log_probs)\n",
    "        old_values = torch.tensor(memory.values)\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Calculate GAE advantage and returns\n",
    "        with torch.no_grad():\n",
    "            _, next_values = policy(next_states)\n",
    "            next_values = next_values.squeeze(-1)\n",
    "            \n",
    "            for t in reversed(range(len(rewards))):\n",
    "                if t == len(rewards) - 1:\n",
    "                    next_value = next_values[t] * (1 - dones[t])\n",
    "                else:\n",
    "                    next_value = old_values[t + 1] * (1 - dones[t])\n",
    "                \n",
    "                delta = rewards[t] + gamma * next_value - old_values[t]\n",
    "                gae = delta + gamma * GAE_LAMBDA * (1 - dones[t]) * gae\n",
    "                advantages.insert(0, gae)\n",
    "                returns.insert(0, gae + old_values[t])\n",
    "        \n",
    "        advantages = torch.tensor(advantages)\n",
    "        returns = torch.tensor(returns)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update\n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            # Evaluate actions\n",
    "            log_probs, entropies, values = [], [], []\n",
    "            for i in range(len(states)):\n",
    "                log_prob, entropy, value = policy.evaluate(states[i], actions[i])\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "                values.append(value)\n",
    "            \n",
    "            log_probs = torch.stack(log_probs)\n",
    "            entropies = torch.stack(entropies)\n",
    "            values = torch.cat(values)\n",
    "            \n",
    "            # Compute ratio and clipped ratio\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1.0 - epsilon_clip, 1.0 + epsilon_clip) * advantages\n",
    "            \n",
    "            # Compute losses\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(values, returns)\n",
    "            entropy_loss = -entropies.mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy_loss\n",
    "            \n",
    "            # Update policy\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        memory.clear()\n",
    "\n",
    "# Train DQN function\n",
    "def train_dqn(memory, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Sample from replay memory\n",
    "    batch = memory.sample(batch_size)\n",
    "    states, actions, next_states, rewards, dones = zip(*batch)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.tensor(actions).unsqueeze(1)\n",
    "    next_states = torch.stack(next_states)\n",
    "    rewards = torch.tensor(rewards).unsqueeze(1)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Calculate current Q values\n",
    "    current_q_values = dqn(states).gather(1, actions)\n",
    "    \n",
    "    # Calculate target Q values\n",
    "    with torch.no_grad():\n",
    "        max_next_q_values = target_dqn(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "    \n",
    "    # Compute loss and update\n",
    "    loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "    \n",
    "    optimizer_dqn.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_dqn.step()\n",
    "\n",
    "# Main game loop\n",
    "def game_loop():\n",
    "    global epsilon\n",
    "    \n",
    "    # Stats and counters\n",
    "    episode = 0\n",
    "    steps = 0\n",
    "    total_red_reward = 0\n",
    "    total_blue_reward = 0\n",
    "    total_green_reward = 0\n",
    "    \n",
    "    # Initialize environment\n",
    "    red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "    \n",
    "    running = True\n",
    "    while running:\n",
    "        # Handle Pygame events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    running = False\n",
    "        \n",
    "        # Get states\n",
    "        red_state = get_mappo_state(red_agent, blue_agent, green_agent, environment)\n",
    "        blue_state = get_mappo_state(blue_agent, red_agent, green_agent, environment)\n",
    "        green_state = get_dqn_state(green_agent, red_agent, blue_agent, environment)\n",
    "        \n",
    "        # Select actions\n",
    "        red_action, red_log_prob, red_value = select_mappo_action(policy_f, red_state)\n",
    "        blue_action, blue_log_prob, blue_value = select_mappo_action(policy_w, blue_state)\n",
    "        green_action = select_dqn_action(green_state, epsilon)\n",
    "        \n",
    "        # Update epsilon for exploration\n",
    "        epsilon = max(EPSILON_END, EPSILON_START - steps / EPSILON_DECAY)\n",
    "        \n",
    "        # Take step in environment\n",
    "        red_reward, blue_reward, green_reward, done, environment = step_environment(\n",
    "            red_agent, blue_agent, green_agent, environment, red_action, blue_action, green_action\n",
    "        )\n",
    "        \n",
    "        # Get next states\n",
    "        next_red_state = get_mappo_state(red_agent, blue_agent, green_agent, environment)\n",
    "        next_blue_state = get_mappo_state(blue_agent, red_agent, green_agent, environment)\n",
    "        next_green_state = get_dqn_state(green_agent, red_agent, blue_agent, environment)\n",
    "        \n",
    "        # Store transitions in memory\n",
    "        memory_f.push(red_state, red_action, red_reward, next_red_state, done, red_log_prob, red_value)\n",
    "        memory_w.push(blue_state, blue_action, blue_reward, next_blue_state, done, blue_log_prob, blue_value)\n",
    "        dqn_memory.push(green_state, green_action, next_green_state, green_reward, done)\n",
    "        \n",
    "        # Train agents\n",
    "        train_mappo([memory_f, memory_w], [policy_f, policy_w], [optimizer_f, optimizer_w], \n",
    "                   GAMMA, PPO_EPSILON, VALUE_COEF, ENTROPY_COEF)\n",
    "        train_dqn(dqn_memory, BATCH_SIZE, GAMMA)\n",
    "        \n",
    "        # Update target network occasionally\n",
    "        steps += 1\n",
    "        if steps % TARGET_UPDATE == 0:\n",
    "            target_dqn.load_state_dict(dqn.state_dict())\n",
    "        \n",
    "        # Draw the grid\n",
    "        screen.fill((0, 0, 0))\n",
    "        draw_grid()\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # Update rewards\n",
    "        total_red_reward += red_reward\n",
    "        total_blue_reward += blue_reward\n",
    "        total_green_reward += green_reward\n",
    "        \n",
    "        # Check if episode is done\n",
    "        if done:\n",
    "            print(f\"Episode {episode} completed!\")\n",
    "            print(f\"Red Reward: {total_red_reward:.2f}, Blue Reward: {total_blue_reward:.2f}, Green Reward: {total_green_reward:.2f}\")\n",
    "            print(f\"Red Gems: {red_agent['collected_gems']}, Blue Gems: {blue_agent['collected_gems']}\")\n",
    "            print(f\"Red Goal: {red_agent['goal_reached']}, Blue Goal: {blue_agent['goal_reached']}\")\n",
    "            print(f\"Red Died: {red_agent['died']}, Blue Died: {blue_agent['died']}\")\n",
    "            \n",
    "            # Reset environment for next episode\n",
    "            red_agent, blue_agent, green_agent, environment = reset_environment()\n",
    "            total_red_reward = total_blue_reward = total_green_reward = 0\n",
    "            episode += 1\n",
    "        \n",
    "        # Cap frame rate\n",
    "        clock.tick(FPS)\n",
    "\n",
    "    pygame.quit()\n",
    "    sys.exit()\n",
    "\n",
    "# Main entry point\n",
    "if __name__ == \"__main__\":\n",
    "    initialize_slides()\n",
    "    game_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7139b34a-35da-445e-a5b9-880f9b4c7ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
