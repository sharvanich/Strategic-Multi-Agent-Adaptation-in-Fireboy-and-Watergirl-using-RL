import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
import pygame
import sys
import numpy as np
from collections import deque
import math

# Constants
TILE_SIZE = 24
GRID_WIDTH = 30
GRID_HEIGHT = 20
SCREEN_WIDTH = TILE_SIZE * GRID_WIDTH
SCREEN_HEIGHT = TILE_SIZE * GRID_HEIGHT
FPS = 10  # Increased FPS for smoother animation
NUM_ACTIONS = 4  # Number of actions (up, down, left, right)

# Initialize Pygame
pygame.init()
screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))
pygame.display.set_caption("Fireboy and Watergirl Grid World")
clock = pygame.time.Clock()

# Colors
COLORS = {
    "0": (120, 66, 18),   # Brown - Empty
    "1": (160, 82, 45),   # Light Brown - Platform
    "F": (255, 0, 0),     # Fireboy - Red Heart
    "W": (0, 0, 255),     # Watergirl - Blue Heart
    "G": (0, 255, 0),     # Green Agent
    "RP": (255, 0, 0),  # Red Poison - Now a block
    "BP": (0, 0, 255),  # Blue Poison - Now a block
    "GP": (0, 255, 0),  # Green Poison - Now a block
    "FG": (255, 0, 0),    # Fireboy Goal
    "WG": (0, 0, 255),    # Watergirl Goal
    "RB": (255, 0, 0),    # Red Gem
    "BB": (0, 0, 255),    # Blue Gem
    "YS": (255, 255, 0),  # Yellow Slide
    "PS": (128, 0, 128),  # Purple Slide
    "YB": (255, 255, 0),  # Yellow Button
    "PB": (128, 0, 128),  # Purple Button
}

BASE_COLOR = COLORS["0"]

# Original grid layout - THIS WILL NEVER BE MODIFIED
ORIGINAL_GRID = [["0" for _ in range(GRID_WIDTH)] for _ in range(GRID_HEIGHT)]

# Place sample tiles
ORIGINAL_GRID[18][1] = "F"    # Fireboy
ORIGINAL_GRID[15][1] = "W"    # Watergirl
ORIGINAL_GRID[2][19] = "G"    # Green agent

# Poison tiles
for x in range(10, 13):
    ORIGINAL_GRID[18][x] = "RP"

for x in range(17, 20):
    ORIGINAL_GRID[18][x] = "BP"

for x in range(18, 21):
    ORIGINAL_GRID[14][x] = "GP"

# Gems
ORIGINAL_GRID[17][11] = ORIGINAL_GRID[8][10] = "RB"
ORIGINAL_GRID[17][18] = ORIGINAL_GRID[10][20] = "BB"

# Goals
ORIGINAL_GRID[2][26] = "FG"
ORIGINAL_GRID[2][28] = "WG"

# Slides
for x in range(1, 4):
    ORIGINAL_GRID[9][x] = "YS"  # Yellow slides

for x in range(26, 29):
    ORIGINAL_GRID[6][x] = "PS"  # Purple slides

# Buttons
ORIGINAL_GRID[8][6] = ORIGINAL_GRID[12][9] = "YB"  # Yellow buttons
ORIGINAL_GRID[8][14] = ORIGINAL_GRID[5][22] = "PB"  # Purple buttons

# Assigning 1's for walls and platforms
for i in range(0, 30):
    ORIGINAL_GRID[0][i] = "1"
    ORIGINAL_GRID[19][i] = "1"

for i in range(0, 20):
    ORIGINAL_GRID[i][0] = "1"
    ORIGINAL_GRID[i][29] = "1"

ORIGINAL_GRID[1][1] = ORIGINAL_GRID[1][2] = "1"
for i in range(4, 29):
    ORIGINAL_GRID[3][i] = "1"

ORIGINAL_GRID[4][1] = ORIGINAL_GRID[4][2] = ORIGINAL_GRID[5][1] = ORIGINAL_GRID[5][2] = "1"

for i in range(1, 26):
    ORIGINAL_GRID[6][i] = "1"

ORIGINAL_GRID[7][18] = ORIGINAL_GRID[7][19] = ORIGINAL_GRID[7][20] = "1"
ORIGINAL_GRID[8][18] = ORIGINAL_GRID[8][19] = ORIGINAL_GRID[8][20] = "1"

for i in range(4, 16):
    ORIGINAL_GRID[9][i] = "1"

for i in range(17, 22):
    ORIGINAL_GRID[11][i] = "1"

for i in range(22, 29):
    ORIGINAL_GRID[10][i] = "1"

ORIGINAL_GRID[10][16] = "1"
ORIGINAL_GRID[11][26] = ORIGINAL_GRID[11][27] = ORIGINAL_GRID[11][28] = "1"
ORIGINAL_GRID[12][26] = ORIGINAL_GRID[12][27] = ORIGINAL_GRID[12][28] = "1"

for i in range(1, 13):
    ORIGINAL_GRID[13][i] = "1"

ORIGINAL_GRID[14][13] = "1"

for i in range(14, 27):
    ORIGINAL_GRID[15][i] = "1"

for i in range(1, 7):
    ORIGINAL_GRID[16][i] = "1"

ORIGINAL_GRID[17][27] = ORIGINAL_GRID[17][28] = ORIGINAL_GRID[18][27] = ORIGINAL_GRID[18][28] = "1"

# Copy original grid to working grid
grid = [[cell for cell in row] for row in ORIGINAL_GRID]

# Track slide positions and active buttons
yellow_slides_positions = []
purple_slides_positions = []
yellow_buttons_active = False
purple_buttons_active = False
slide_animation_counter = 0
SLIDE_ANIMATION_SPEED = 15  # Controls how fast slides move

# Initialize slide positions
def initialize_slides():
    global yellow_slides_positions, purple_slides_positions
    yellow_slides_positions = []
    purple_slides_positions = []
    
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            if ORIGINAL_GRID[y][x] == "YS":
                yellow_slides_positions.append((y, x))
            elif ORIGINAL_GRID[y][x] == "PS":
                purple_slides_positions.append((y, x))

# Create grid maps for environment features
def create_grid_map(feature_type):
    """Create a boolean grid map for different environmental features"""
    grid_map = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=bool)
    
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            cell = grid[y][x]
            if feature_type == 'wall' and cell == "1":
                grid_map[y][x] = True
            elif feature_type == 'red_poison' and cell == "RP":
                grid_map[y][x] = True
            elif feature_type == 'blue_poison' and cell == "BP":
                grid_map[y][x] = True
            elif feature_type == 'green_poison' and cell == "GP":
                grid_map[y][x] = True
            elif feature_type == 'red_gem' and cell == "RB":
                grid_map[y][x] = True
            elif feature_type == 'blue_gem' and cell == "BB":
                grid_map[y][x] = True
            elif feature_type == 'fire_goal' and cell == "FG":
                grid_map[y][x] = True
            elif feature_type == 'water_goal' and cell == "WG":
                grid_map[y][x] = True
            elif feature_type == 'yellow_button' and cell == "YB":
                grid_map[y][x] = True
            elif feature_type == 'purple_button' and cell == "PB":
                grid_map[y][x] = True
            elif feature_type == 'yellow_slide' and cell == "YS":
                grid_map[y][x] = True
            elif feature_type == 'purple_slide' and cell == "PS":
                grid_map[y][x] = True
    
    return grid_map

# Create all grid maps
def initialize_environment_maps():
    wall_map = create_grid_map('wall')
    red_poison_map = create_grid_map('red_poison')
    blue_poison_map = create_grid_map('blue_poison')
    green_poison_map = create_grid_map('green_poison')
    red_gem_map = create_grid_map('red_gem')
    blue_gem_map = create_grid_map('blue_gem')
    fire_goal_map = create_grid_map('fire_goal')
    water_goal_map = create_grid_map('water_goal')
    yellow_button_map = create_grid_map('yellow_button')
    purple_button_map = create_grid_map('purple_button')
    yellow_slide_map = create_grid_map('yellow_slide')
    purple_slide_map = create_grid_map('purple_slide')
    
    # Create combined maps
    poison_map = red_poison_map | blue_poison_map | green_poison_map
    
    # Create environment dictionary
    environment = {
        'wall': wall_map,
        'poison': poison_map,
        'red_poison': red_poison_map,
        'blue_poison': blue_poison_map,
        'green_poison': green_poison_map,
        'red_gems': red_gem_map.copy(),
        'blue_gems': blue_gem_map.copy(),
        'fire_goal': fire_goal_map,
        'water_goal': water_goal_map,
        'yellow_button': yellow_button_map,
        'purple_button': purple_button_map,
        'yellow_slide': yellow_slide_map,
        'purple_slide': purple_slide_map
    }
    
    return environment

# Agent position setup
def get_initial_positions():
    fireboy_pos = None
    watergirl_pos = None
    green_pos = None
    
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            if grid[y][x] == "F":
                fireboy_pos = (y, x)
            elif grid[y][x] == "W":
                watergirl_pos = (y, x)
            elif grid[y][x] == "G":
                green_pos = (y, x)
    
    return fireboy_pos, watergirl_pos, green_pos

# Draw Function
def draw_grid():
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            tile = grid[y][x]
            color = COLORS.get(tile, BASE_COLOR)

            # Agents: Heart shape
            if tile in ["F", "W", "G"]:
                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))
                pygame.draw.polygon(screen, color, [
                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + 6),
                    (x*TILE_SIZE + 6, y*TILE_SIZE + TILE_SIZE//2),
                    (x*TILE_SIZE + TILE_SIZE - 6, y*TILE_SIZE + TILE_SIZE//2),
                ])
                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)
                pygame.draw.circle(screen, color, (x*TILE_SIZE + 2*TILE_SIZE//3, y*TILE_SIZE + TILE_SIZE//3), 5)

            # Gems: Rhombus
            elif tile in ["RB", "BB"]:
                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))
                pygame.draw.polygon(screen, color, [
                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),
                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2),
                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE),
                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE//2)
                ])

            # Buttons: Circle
            elif tile in ["YB", "PB"]:
                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))
                pygame.draw.circle(screen, color, (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE + TILE_SIZE//2), TILE_SIZE//4)

            # Goals: Triangle
            elif tile in ["FG", "WG"]:
                pygame.draw.rect(screen, BASE_COLOR, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))
                pygame.draw.polygon(screen, color, [
                    (x*TILE_SIZE + TILE_SIZE//2, y*TILE_SIZE),
                    (x*TILE_SIZE, y*TILE_SIZE + TILE_SIZE),
                    (x*TILE_SIZE + TILE_SIZE, y*TILE_SIZE + TILE_SIZE)
                ])

            # Slides: Full tile fill
            elif tile in ["YS", "PS"]:
                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))
                
            # Poison: Now blocks with specific colors
            elif tile in ["RP", "BP", "GP"]:
                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))

            # Default or walls
            else:
                pygame.draw.rect(screen, color, (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE))

            # Draw the black border
            pygame.draw.rect(screen, (0, 0, 0), (x*TILE_SIZE, y*TILE_SIZE, TILE_SIZE, TILE_SIZE), 1)

# Enhanced neural network for MAPPO
class MAPPONetwork(nn.Module):
    def __init__(self, input_size, hidden_size, action_size):
        super(MAPPONetwork, self).__init__()
        # Actor network (policy)
        self.actor = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_size),
        )
        
        # Critic network (value function)
        self.critic = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )
    
    def forward(self, state):
        # Use log_softmax for better numerical stability
        actor_output = self.actor(state)
        # Add small epsilon to prevent log(0)
        probs = F.softmax(actor_output, dim=-1) + 1e-10
        # Renormalize
        probs = probs / probs.sum()
        return probs, self.critic(state)
    
    def get_action(self, state, deterministic=False):
        probs, _ = self.forward(state)
        if deterministic:
            return torch.argmax(probs).item()
        else:
            return torch.multinomial(probs, 1).item()
    
    def evaluate(self, state, action):
        probs, value = self.forward(state)
        # Add safety check for NaN values in probs
        if torch.isnan(probs).any():
            probs = torch.ones_like(probs) / probs.size(-1)  # Equal probabilities as fallback
        dist = torch.distributions.Categorical(probs)
        log_prob = dist.log_prob(action)
        entropy = dist.entropy()
        return log_prob, entropy, value

# Define the DQN for the green agent
class DQN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)  # Q-values (not softmax)

# Memory buffer for MAPPO
class MAPPOMemory:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.next_states = []
        self.dones = []
        self.log_probs = []
        self.values = []
    
    def push(self, state, action, reward, next_state, done, log_prob, value):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.next_states.append(next_state)
        self.dones.append(done)
        self.log_probs.append(log_prob)
        self.values.append(value)
    
    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.next_states.clear()
        self.dones.clear()
        self.log_probs.clear()
        self.values.clear()
    
    def __len__(self):
        return len(self.states)

# Experience replay memory for DQN
class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque(maxlen=capacity)
    
    def push(self, state, action, next_state, reward, done):
        self.memory.append((state, action, next_state, reward, done))
    
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

# Initialize neural networks with proper hidden sizes
HIDDEN_SIZE = 128
INPUT_SIZE_MAPPO = 16  # Expanded input size for better state representation
# FIX: Match the input size to the actual features in the state
INPUT_SIZE_DQN = 23    # Updated to match actual state size from get_dqn_state

# Initialize MAPPO networks for cooperative agents
policy_f = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)
policy_w = MAPPONetwork(INPUT_SIZE_MAPPO, HIDDEN_SIZE, NUM_ACTIONS)

# Initialize DQN for the green adversarial agent
dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)
target_dqn = DQN(INPUT_SIZE_DQN, HIDDEN_SIZE, NUM_ACTIONS)
target_dqn.load_state_dict(dqn.state_dict())  # Initially the same as the online network

LEARNING_RATE = 0.003
GAMMA = 0.99  # Discount factor

# Optimizers
optimizer_f = optim.Adam(policy_f.parameters(), lr=LEARNING_RATE)
optimizer_w = optim.Adam(policy_w.parameters(), lr=LEARNING_RATE)
optimizer_dqn = optim.Adam(dqn.parameters(), lr=LEARNING_RATE)

# MAPPO memory buffers
memory_f = MAPPOMemory()
memory_w = MAPPOMemory()

# DQN experience replay memory
dqn_memory = ReplayMemory(10000)

# DQN hyperparameters
BATCH_SIZE = 64
EPSILON_START = 1.0
EPSILON_END = 0.1
EPSILON_DECAY = 10000
TARGET_UPDATE = 10  # How often to update target network
epsilon = EPSILON_START

# MAPPO hyperparameters
PPO_EPOCHS = 8
PPO_EPSILON = 0.2  # Clip parameter
VALUE_COEF = 0.5
ENTROPY_COEF = 0.02
GAE_LAMBDA = 0.95

# Initialize agent state
def reset_environment():
    # Reset grid to original state first
    global grid, yellow_buttons_active, purple_buttons_active, slide_animation_counter
    grid = [[cell for cell in row] for row in ORIGINAL_GRID]
    yellow_buttons_active = False
    purple_buttons_active = False
    slide_animation_counter = 0
    
    # Initialize slides
    initialize_slides()
    
    # Get fresh positions from the reset grid
    fireboy_pos, watergirl_pos, green_pos = get_initial_positions()
    
    # Create agent dictionaries with position and other properties
    red_agent = {'pos': fireboy_pos, 'color': 'red', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}
    blue_agent = {'pos': watergirl_pos, 'color': 'blue', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}
    green_agent = {'pos': green_pos, 'color': 'green', 'is_pressing_button': False, 'goal_reached': False, 'collected_gems': 0, 'died': False}
    
    # Create fresh environment maps from the reset grid
    environment = initialize_environment_maps()
    
    return red_agent, blue_agent, green_agent, environment

# Handle slide movement
def update_slides():
    global yellow_slides_positions, purple_slides_positions, grid, slide_animation_counter
    
    # Clear previous slide positions from grid
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            if grid[y][x] == "YS" or grid[y][x] == "PS":
                grid[y][x] = "0"
    
    # Update yellow slides
    new_yellow_positions = []
    for y, x in yellow_slides_positions:
        # If button is pressed, move down 3 rows, otherwise stay in original position
        if yellow_buttons_active:
            target_y = min(y + 3, GRID_HEIGHT - 1)
            # Check if we're in animation
            if slide_animation_counter < SLIDE_ANIMATION_SPEED:
                # Calculate intermediate position
                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED
                current_y = int(y + (target_y - y) * animation_progress)
                new_yellow_positions.append((current_y, x))
                grid[current_y][x] = "YS"
            else:
                new_yellow_positions.append((target_y, x))
                grid[target_y][x] = "YS"
        else:
            # If buttons not active, return to original position
            if slide_animation_counter < SLIDE_ANIMATION_SPEED:
                original_y = [pos[0] for pos in yellow_slides_positions if pos[1] == x][0]
                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED
                # Find the current position in the animation
                if any(pos[1] == x for pos in yellow_slides_positions):
                    current_pos = [pos for pos in yellow_slides_positions if pos[1] == x][0]
                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)
                    new_yellow_positions.append((current_y, x))
                    grid[current_y][x] = "YS"
            else:
                new_yellow_positions.append((y, x))
                grid[y][x] = "YS"
    
    # Update purple slides
    new_purple_positions = []
    for y, x in purple_slides_positions:
        # If button is pressed, move down 3 rows, otherwise stay in original position
        if purple_buttons_active:
            target_y = min(y + 3, GRID_HEIGHT - 1)
            # Check if we're in animation
            if slide_animation_counter < SLIDE_ANIMATION_SPEED:
                # Calculate intermediate position
                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED
                current_y = int(y + (target_y - y) * animation_progress)
                new_purple_positions.append((current_y, x))
                grid[current_y][x] = "PS"
            else:
                new_purple_positions.append((target_y, x))
                grid[target_y][x] = "PS"
        else:
            # If buttons not active, return to original position
            if slide_animation_counter < SLIDE_ANIMATION_SPEED:
                original_y = [pos[0] for pos in purple_slides_positions if pos[1] == x][0]
                animation_progress = slide_animation_counter / SLIDE_ANIMATION_SPEED
                # Find the current position in the animation
                if any(pos[1] == x for pos in purple_slides_positions):
                    current_pos = [pos for pos in purple_slides_positions if pos[1] == x][0]
                    current_y = int(current_pos[0] - (current_pos[0] - original_y) * animation_progress)
                    new_purple_positions.append((current_y, x))
                    grid[current_y][x] = "PS"
            else:
                new_purple_positions.append((y, x))
                grid[y][x] = "PS"
    
    # Update slide positions for next frame
    yellow_slides_positions = new_yellow_positions
    purple_slides_positions = new_purple_positions
    
    # Update animation counter
    if slide_animation_counter < SLIDE_ANIMATION_SPEED:
        slide_animation_counter += 1

# Enhanced state representation for MAPPO
def get_mappo_state(agent, other_agent, green_agent, environment):
    y, x = agent['pos']
    oy, ox = other_agent['pos']
    gy, gx = green_agent['pos']
    
    # Find locations of goals
    fire_goal_positions = np.where(environment['fire_goal'])
    water_goal_positions = np.where(environment['water_goal'])
    # Safer way to get goal positions
    if len(fire_goal_positions[0]) > 0:
        fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0]
    else:
        fgy, fgx = 0, 0
    
    if len(water_goal_positions[0]) > 0:
        wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0]
    else:
        wgy, wgx = 0, 0
        
    # Target goal based on agent color
    goal_y, goal_x = (fgy, fgx) if agent['color'] == 'red' else (wgy, wgx)
    
    # Calculate distances
    dist_to_goal = np.sqrt((y - goal_y)**2 + (x - goal_x)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    dist_to_other = np.sqrt((y - oy)**2 + (x - ox)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    dist_to_green = np.sqrt((y - gy)**2 + (x - gx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    
    # Comprehensive state representation
    state = [
        y / GRID_HEIGHT,                  # Normalized y position
        x / GRID_WIDTH,                   # Normalized x position
        oy / GRID_HEIGHT,                 # Other agent y position
        ox / GRID_WIDTH,                  # Other agent x position
        gy / GRID_HEIGHT,                 # Green agent y position
        gx / GRID_WIDTH,                  # Green agent x position
        dist_to_goal,                     # Distance to own goal
        dist_to_other,                    # Distance to other agent
        dist_to_green,                    # Distance to green agent
        1.0 if agent['is_pressing_button'] else 0.0,  # Is agent pressing button
        1.0 if other_agent['is_pressing_button'] else 0.0,  # Is other agent pressing button
        1.0 if yellow_buttons_active else 0.0,        # Is yellow button active
        1.0 if purple_buttons_active else 0.0,        # Is purple button active
        1.0 if agent['goal_reached'] else 0.0,        # Has agent reached goal
        agent['collected_gems'] / 2.0,                # Normalized collected gems
        other_agent['collected_gems'] / 2.0           # Other agent's collected gems
    ]
    
    return torch.tensor(state, dtype=torch.float)

# Enhanced state representation for DQN (green agent)
def get_dqn_state(green_agent, red_agent, blue_agent, environment):
    gy, gx = green_agent['pos']
    ry, rx = red_agent['pos']
    by, bx = blue_agent['pos']
    
    # Find locations of goals
    fire_goal_positions = np.where(environment['fire_goal'])
    water_goal_positions = np.where(environment['water_goal'])
    # Similarly update the goal position extraction in get_dqn_state:
    if len(fire_goal_positions[0]) > 0:
        fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0]
    else:
        fgy, fgx = 0, 0
    
    if len(water_goal_positions[0]) > 0:
        wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0]
    else:
        wgy, wgx = 0, 0
    
    # Distance calculations
    dist_to_red = np.sqrt((gy - ry)**2 + (gx - rx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    dist_to_blue = np.sqrt((gy - by)**2 + (gx - bx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    dist_to_fire_goal = np.sqrt((gy - fgy)**2 + (gx - fgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    dist_to_water_goal = np.sqrt((gy - wgy)**2 + (gx - wgx)**2) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2)
    
    # Get button information
    yellow_button_positions = np.where(environment['yellow_button'])
    purple_button_positions = np.where(environment['purple_button'])
    
    # Find distances to buttons
    dist_to_yellow_button = min([np.sqrt((gy - y)**2 + (gx - x)**2) for y, x in zip(yellow_button_positions[0], yellow_button_positions[1])]) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2) if len(yellow_button_positions[0]) > 0 else 1.0
    dist_to_purple_button = min([np.sqrt((gy - y)**2 + (gx - x)**2) for y, x in zip(purple_button_positions[0], purple_button_positions[1])]) / np.sqrt(GRID_HEIGHT**2 + GRID_WIDTH**2) if len(purple_button_positions[0]) > 0 else 1.0
    
    # Check surroundings (4 adjacent cells)
    directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]
    surroundings = []
    for dy, dx in directions:
        ny, nx = gy + dy, gx + dx
        if 0 <= ny < GRID_HEIGHT and 0 <= nx < GRID_WIDTH:
            if environment['wall'][ny][nx]:
                surroundings.append(1.0)  # Wall
            elif (ny, nx) == (ry, rx):
                surroundings.append(2.0)  # Red agent
            elif (ny, nx) == (by, bx):
                surroundings.append(3.0)  # Blue agent
            else:
                surroundings.append(0.0)  # Empty
        else:
            surroundings.append(1.0)  # Out of bounds (treat as wall)
    
    # Comprehensive state representation for the green agent
    state = [
        gy / GRID_HEIGHT,                  # Normalized y position
        gx / GRID_WIDTH,                   # Normalized x position
        ry / GRID_HEIGHT,                  # Red agent y position
        rx / GRID_WIDTH,                   # Red agent x position
        by / GRID_HEIGHT,                  # Blue agent y position
        bx / GRID_WIDTH,                   # Blue agent x position
        dist_to_red,                       # Distance to red agent
        dist_to_blue,                      # Distance to blue agent
        dist_to_fire_goal,                 # Distance to fire goal
        dist_to_water_goal,                # Distance to water goal
        dist_to_yellow_button,             # Distance to closest yellow button
        dist_to_purple_button,             # Distance to closest purple button
        1.0 if yellow_buttons_active else 0.0,  # Is yellow button active
        1.0 if purple_buttons_active else 0.0,  # Is purple button active
        1.0 if green_agent['is_pressing_button'] else 0.0,  # Is green agent pressing button
        1.0 if red_agent['goal_reached'] else 0.0,  # Has red agent reached goal
        1.0 if blue_agent['goal_reached'] else 0.0,  # Has blue agent reached goal
        red_agent['collected_gems'] / 2.0,  # Red agent's collected gems
        blue_agent['collected_gems'] / 2.0,  # Blue agent's collected gems
    ] + surroundings  # Add surroundings (4 values)
    
    return torch.tensor(state, dtype=torch.float)

# Get actions and update agent positions
def step_environment(red_agent, blue_agent, green_agent, environment, red_action, blue_action, green_action):
    # Process button presses first (from previous state)
    check_button_presses(red_agent, blue_agent, green_agent, environment)
    
    # Update slide positions based on button activity
    update_slides()
    
    # Update environment maps after potential slide movement
    environment = initialize_environment_maps()
    
    # Process agent movements based on actions
    red_done = move_agent(red_agent, environment, red_action)
    blue_done = move_agent(blue_agent, environment, blue_action)
    green_done = move_agent(green_agent, environment, green_action)
    
    # Check collectibles and hazards after movement
    red_reward = check_collectibles_and_hazards(red_agent, environment)
    blue_reward = check_collectibles_and_hazards(blue_agent, environment)
    green_reward = check_collectibles_and_hazards(green_agent, environment)
    
    # Check if agents reached their goals
    red_reached_goal = check_goal(red_agent, environment, 'fire_goal')
    blue_reached_goal = check_goal(blue_agent, environment, 'water_goal')
    
    # Calculate reward based on game state
    # For cooperative agents (red and blue) - IMPROVED REWARDS
    if red_agent['died'] or blue_agent['died']:
        red_reward -= 10
        blue_reward -= 10
    
    # Calculate distances to goals for reward shaping
    ry, rx = red_agent['pos']
    by, bx = blue_agent['pos']
    
    # Find locations of goals
    fire_goal_positions = np.where(environment['fire_goal'])
    water_goal_positions = np.where(environment['water_goal'])
    
    if len(fire_goal_positions[0]) > 0:
        fgy, fgx = fire_goal_positions[0][0], fire_goal_positions[1][0]
        # Reward for getting closer to goal
        red_dist_to_goal = np.sqrt((ry - fgy)**2 + (rx - fgx)**2)
        red_reward += (1 / (red_dist_to_goal + 1)) * 0.5  # Reward inversely proportional to distance
    
    if len(water_goal_positions[0]) > 0:
        wgy, wgx = water_goal_positions[0][0], water_goal_positions[1][0]
        # Reward for getting closer to goal
        blue_dist_to_goal = np.sqrt((by - wgy)**2 + (bx - wgx)**2)
        blue_reward += (1 / (blue_dist_to_goal + 1)) * 0.5  # Reward inversely proportional to distance
    
    if red_reached_goal:
        red_agent['goal_reached'] = True
        red_reward += 30  # Increased from 20
        blue_reward += 15  # Increased from 10
    
    if blue_reached_goal:
        blue_agent['goal_reached'] = True
        blue_reward += 30  # Increased from 20
        red_reward += 15  # Increased from 10
    
    # Both reached goal is a big win
    if red_agent['goal_reached'] and blue_agent['goal_reached']:
        red_reward += 100  # Increased from 50
        blue_reward += 100  # Increased from 50
    
    # For adversarial agent (green)
    if red_agent['died'] or blue_agent['died']:
        green_reward += 15
    
    if red_agent['goal_reached'] or blue_agent['goal_reached']:
        green_reward -= 10
    
    if red_agent['goal_reached'] and blue_agent['goal_reached']:
        green_reward -= 30
    
    # Check if episode is done
    done = (red_agent['died'] or blue_agent['died'] or 
            (red_agent['goal_reached'] and blue_agent['goal_reached']) or
            red_done or blue_done or green_done)
    
    # Update grid representation for visualization
    update_grid_representation(red_agent, blue_agent, green_agent)
    
    return red_reward, blue_reward, green_reward, done, environment

# Check if agent is on a button
def check_button_presses(red_agent, blue_agent, green_agent, environment):
    global yellow_buttons_active, purple_buttons_active, slide_animation_counter
    
    # Reset button states
    yellow_buttons_pressed = False
    purple_buttons_pressed = False
    
    # Check each agent
    for agent in [red_agent, blue_agent, green_agent]:
        y, x = agent['pos']
        if environment['yellow_button'][y][x]:
            yellow_buttons_pressed = True
            agent['is_pressing_button'] = True
        elif environment['purple_button'][y][x]:
            purple_buttons_pressed = True
            agent['is_pressing_button'] = True
        else:
            agent['is_pressing_button'] = False
    
    # If button state changed, reset animation counter
    if yellow_buttons_pressed != yellow_buttons_active or purple_buttons_pressed != purple_buttons_active:
        slide_animation_counter = 0
    
    yellow_buttons_active = yellow_buttons_pressed
    purple_buttons_active = purple_buttons_pressed

# Move agent based on action
def move_agent(agent, environment, action):
    y, x = agent['pos']
    
    if action == 0:  # UP
        y -= 1
    elif action == 1:  # RIGHT
        x += 1
    elif action == 2:  # DOWN
        y += 1
    elif action == 3:  # LEFT
        x -= 1
    
    # Check if new position is valid
    if 0 <= y < GRID_HEIGHT and 0 <= x < GRID_WIDTH and not environment['wall'][y][x]:
        agent['pos'] = (y, x)
    
    # Check if agent is out of bounds or in an invalid position
    return not (0 <= y < GRID_HEIGHT and 0 <= x < GRID_WIDTH)

# Check for collectibles and hazards
def check_collectibles_and_hazards(agent, environment):
    y, x = agent['pos']
    reward = 0
    
    # Check for gems - MODIFIED to ensure red agent only accesses red gems
    if agent['color'] == 'red' and environment['red_gems'][y][x]:
        environment['red_gems'][y][x] = False
        agent['collected_gems'] += 1
        reward += 5
        # Clear gem from grid for visualization
        grid[y][x] = "0"
    
    elif agent['color'] == 'blue' and environment['blue_gems'][y][x]:
        environment['blue_gems'][y][x] = False
        agent['collected_gems'] += 1
        reward += 5
        # Clear gem from grid for visualization
        grid[y][x] = "0"
    
    # Check for poison (specific to agent type)
    if (agent['color'] == 'red' and environment['blue_poison'][y][x]) or \
       (agent['color'] == 'blue' and environment['red_poison'][y][x]) or \
       (agent['color'] in ['red', 'blue', 'green'] and environment['green_poison'][y][x]):
        agent['died'] = True
        reward -= 20
        # Agents no longer disappear when touching poison - they're just marked as 'died'
    
    return reward

# Check if agent reached goal
def check_goal(agent, environment, goal_type):
    y, x = agent['pos']
    if environment[goal_type][y][x]:
        return True
    return False

# Update grid representation for visualization
def update_grid_representation(red_agent, blue_agent, green_agent):
    # Clear agent positions from grid
    for y in range(GRID_HEIGHT):
        for x in range(GRID_WIDTH):
            if grid[y][x] in ["F", "W", "G"]:
                grid[y][x] = "0"
    
    # Add agents to grid - even if they're dead, they'll still be visible
    ry, rx = red_agent['pos']
    by, bx = blue_agent['pos']
    gy, gx = green_agent['pos']
    
    grid[ry][rx] = "F"
    grid[by][bx] = "W"
    grid[gy][gx] = "G"

# Select action using MAPPO policies (with exploration)
def select_mappo_action(policy, state, deterministic=False):
    with torch.no_grad():
        action_probs, value = policy(state)
        if deterministic:
            action = torch.argmax(action_probs).item()
        else:
            action = torch.multinomial(action_probs, 1).item()
        log_prob = torch.log(action_probs[action])
        
    return action, log_prob.item(), value.item()

# Select action using DQN (with epsilon-greedy exploration)
def select_dqn_action(state, epsilon):
    if random.random() < epsilon:
        return random.randint(0, NUM_ACTIONS - 1)
    else:
        with torch.no_grad():
            q_values = dqn(state)
            return torch.argmax(q_values).item()

# Training functions
def train_mappo(memories, policies, optimizers, gamma, epsilon_clip, value_coef, entropy_coef):
    for memory, policy, optimizer in zip(memories, policies, optimizers):
        if len(memory) == 0:
            continue
        
        # Convert lists to tensors and ensure float32 dtype
        states = torch.stack(memory.states).float()
        actions = torch.tensor(memory.actions).float()
        rewards = torch.tensor(memory.rewards).float()
        next_states = torch.stack(memory.next_states).float()
        dones = torch.tensor(memory.dones).float()
        old_log_probs = torch.tensor(memory.log_probs).float()
        old_values = torch.tensor(memory.values).float()
        
        # Compute returns and advantages
        returns = []
        advantages = []
        gae = 0
        
        # Calculate GAE advantage and returns
        with torch.no_grad():
            _, next_values = policy(next_states)
            next_values = next_values.squeeze(-1)
            
            for t in reversed(range(len(rewards))):
                if t == len(rewards) - 1:
                    next_value = next_values[t] * (1 - dones[t])
                else:
                    next_value = old_values[t + 1] * (1 - dones[t])
                
                delta = rewards[t] + gamma * next_value - old_values[t]
                gae = delta + gamma * GAE_LAMBDA * (1 - dones[t]) * gae
                advantages.insert(0, gae)
                returns.insert(0, gae + old_values[t])
        
        advantages = torch.tensor(advantages)
        returns = torch.tensor(returns)

        # Normalize advantages for stability
        if len(advantages) > 1 and advantages.std() > 1e-8:
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # PPO update
        for _ in range(PPO_EPOCHS):
            # Evaluate actions
            log_probs, entropies, values = [], [], []
            for i in range(len(states)):
                log_prob, entropy, value = policy.evaluate(states[i], actions[i])
                log_probs.append(log_prob)
                entropies.append(entropy)
                values.append(value)
            
            log_probs = torch.stack(log_probs)
            entropies = torch.stack(entropies)
            values = torch.cat(values)
            
            # Compute ratio and clipped ratio
            ratios = torch.exp(log_probs - old_log_probs)
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1.0 - epsilon_clip, 1.0 + epsilon_clip) * advantages
            
            # Compute losses
            policy_loss = -torch.min(surr1, surr2).mean()
            value_loss = F.mse_loss(values, returns)
            entropy_loss = -entropies.mean()
            
            # Total loss
            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy_loss
            
            # Update policy
            optimizer.zero_grad()
            loss.backward()
            # Add gradient clipping to prevent exploding gradients
            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)
            optimizer.step()
        
        # Clear memory
        for memory in memories:
            memory.clear()

# Train DQN function
def train_dqn(memory, batch_size, gamma):
    if len(memory) < batch_size:
        return
    
    # Sample from replay memory
    batch = memory.sample(batch_size)
    states, actions, next_states, rewards, dones = zip(*batch)
    
    # Convert to tensors
    states = torch.stack(states)
    actions = torch.tensor(actions).unsqueeze(1)
    next_states = torch.stack(next_states)
    rewards = torch.tensor(rewards).unsqueeze(1)
    dones = torch.tensor(dones, dtype=torch.float).unsqueeze(1)
    
    # Calculate current Q values
    current_q_values = dqn(states).gather(1, actions)
    
    # Calculate target Q values
    with torch.no_grad():
        max_next_q_values = target_dqn(next_states).max(1)[0].unsqueeze(1)
        target_q_values = rewards + (gamma * max_next_q_values * (1 - dones))
    
    # Compute loss and update
    loss = F.smooth_l1_loss(current_q_values, target_q_values)
    
    optimizer_dqn.zero_grad()
    loss.backward()
    optimizer_dqn.step()

# Main game loop
def game_loop():
    global epsilon
    
    # Stats and counters
    episode = 0
    steps = 0
    total_red_reward = 0
    total_blue_reward = 0
    total_green_reward = 0
    
    # Initialize environment
    red_agent, blue_agent, green_agent, environment = reset_environment()
    
    running = True
    while running:
        # Handle Pygame events
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            elif event.type == pygame.KEYDOWN:
                if event.key == pygame.K_ESCAPE:
                    running = False
        
        # Get states
        red_state = get_mappo_state(red_agent, blue_agent, green_agent, environment)
        blue_state = get_mappo_state(blue_agent, red_agent, green_agent, environment)
        green_state = get_dqn_state(green_agent, red_agent, blue_agent, environment)
        
        # Select actions
        red_action, red_log_prob, red_value = select_mappo_action(policy_f, red_state)
        blue_action, blue_log_prob, blue_value = select_mappo_action(policy_w, blue_state)
        green_action = select_dqn_action(green_state, epsilon)
        
        # Update epsilon for exploration
        epsilon = max(EPSILON_END, EPSILON_START - steps / EPSILON_DECAY)
        
        # Take step in environment
        red_reward, blue_reward, green_reward, done, environment = step_environment(
            red_agent, blue_agent, green_agent, environment, red_action, blue_action, green_action
        )
        
        # Get next states
        next_red_state = get_mappo_state(red_agent, blue_agent, green_agent, environment)
        next_blue_state = get_mappo_state(blue_agent, red_agent, green_agent, environment)
        next_green_state = get_dqn_state(green_agent, red_agent, blue_agent, environment)
        
        # Store transitions in memory
        memory_f.push(red_state, red_action, red_reward, next_red_state, done, red_log_prob, red_value)
        memory_w.push(blue_state, blue_action, blue_reward, next_blue_state, done, blue_log_prob, blue_value)
        dqn_memory.push(green_state, green_action, next_green_state, green_reward, done)
        
        # Train agents
        train_mappo([memory_f, memory_w], [policy_f, policy_w], [optimizer_f, optimizer_w], GAMMA, PPO_EPSILON, VALUE_COEF, ENTROPY_COEF)
        train_dqn(dqn_memory, BATCH_SIZE, GAMMA)
        
        # Update target network occasionally
        steps += 1
        if steps % TARGET_UPDATE == 0:
            target_dqn.load_state_dict(dqn.state_dict())
        
        # Draw the grid
        screen.fill((0, 0, 0))
        draw_grid()
        pygame.display.flip()
        
        # Update rewards
        total_red_reward += red_reward
        total_blue_reward += blue_reward
        total_green_reward += green_reward
        
        # Check if episode is done
        if done:
            print(f"Episode {episode} completed!")
            print(f"Red Reward: {total_red_reward:.2f}, Blue Reward: {total_blue_reward:.2f}, Green Reward: {total_green_reward:.2f}")
            print(f"Red Gems: {red_agent['collected_gems']}, Blue Gems: {blue_agent['collected_gems']}")
            print(f"Red Goal: {red_agent['goal_reached']}, Blue Goal: {blue_agent['goal_reached']}")
            print(f"Red Died: {red_agent['died']}, Blue Died: {blue_agent['died']}")
            
            # Reset environment for next episode
            red_agent, blue_agent, green_agent, environment = reset_environment()
            total_red_reward = total_blue_reward = total_green_reward = 0
            episode += 1
        
        # Cap frame rate
        clock.tick(FPS)

    pygame.quit()
    sys.exit()

# Main entry point
if __name__ == "__main__":
    initialize_slides()
    game_loop()
